---
title: "Course Notes"
format: pdf
editor: source
---

# Lecture 1

## Data preprocessing

Common computing paradigms: Rules + Data = Answers (Through Computing)

Machine Learning: Data + Answers = Rules (Learning!)

Using data is a process that repeats in a cycle:

1. Business understanding
2. Data understanding
    - Refer back to **1.** to ensure cohesiveness
    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)
3. Data preparation
4. Modeling
    - Refer back to **3.** to ensure cohesiveness
5. Evaluation
    - Return back to **1.** if needed
6. Deployment

**Why do we want to clean data?**

- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)
- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them
- Missing values **should be deleted** when there are too many missing values
- Missing values **can be replaced** when you can estimate them using imputation procedures
    - For *continuous variables*, replace with median or mean (median more robust to outliers)
    - For *nominal variables*, replace with mode
    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage
- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value
    - Some may be hidden in one-dimensional views of multidimensional data
    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy
- **Valid outliers** and their treatments
    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\pm$ 3 standard deviations
    - Truncation based on inter-quartile range: replace all variable values with $Median \pm 3s, \quad \text{ where s }= IQR/(2 \times 0.6745)$
    - Truncation using a sigmoid

## Statistical models

### Input data

There are multiple types of numerical data

- Continuous, categorical, integer

In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$

### Target Variables

The core of *supervised learning* is that there is a variable $y$ representing the outcome

- Continuous $y$ is a supervised regression problem
- Discrete $y$ is a supervised classification problem
- Binary $y$ is the easiest version of a supervised classification problem

If no set $Y$ outcomes exist, then the problem is *unsupervised*

## Defining a model and losses

A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)

**Mean-square error**: $L(Y,\hat{Y}) = \sum_i (y_i - \hat{y}_i)^2$

- Appropriate for regression analysis

**Cross entropy**: 

$l(y_i, \hat{y}_i) = -y_ilog(\hat{y}_i) - (1-y_i)log(1-\hat{y}_i)$ for a binary version

$l(y_i, \hat{y}_i) = - \sum_k y_i^k log\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors

### Regression functions

The "best" model is the one that minimizes the expected error over the full distribution of the set of input and output

The problem, mathematically, becomes $min_\beta E[L(Y,f(x|\beta))]$

**Linear regression example**

For the whole sample:

$$Y = X\beta + \epsilon$$

We can then take the least-squares to find the model for our $\beta$

