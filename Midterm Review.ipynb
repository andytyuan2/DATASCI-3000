{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,cross_validate, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Scipy packages\n",
    "from scipy.stats import zscore\n",
    "from scipy.special import factorial\n",
    "import scipy.optimize as so\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Common computing paradigms: Rules + Data = Answers (Through Computing)\n",
    "\n",
    "Machine Learning: Data + Answers = Rules (Learning!)\n",
    "\n",
    "Using data is a process that repeats in a cycle:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "    - Refer back to **1.** to ensure cohesiveness\n",
    "    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "    - Refer back to **3.** to ensure cohesiveness\n",
    "5. Evaluation\n",
    "    - Return back to **1.** if needed\n",
    "6. Deployment\n",
    "\n",
    "Steps to cleaning data:\n",
    "\n",
    "1. Create variables\n",
    "2. Eliminate redundant variables\n",
    "3. Treat null values\n",
    "4. Treat outliers as valid or invalid\n",
    "\n",
    "**Why do we want to clean data?**\n",
    "\n",
    "- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)\n",
    "- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them\n",
    "- Missing values **should be deleted** when there are too many missing values\n",
    "- Missing values **can be replaced** when you can estimate them using imputation procedures\n",
    "    - For *continuous variables*, replace with median or mean (median more robust to outliers)\n",
    "    - For *nominal variables*, replace with mode\n",
    "    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage\n",
    "- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value\n",
    "    - Some may be hidden in one-dimensional views of multidimensional data\n",
    "    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy\n",
    "- **Valid outliers** and their treatments\n",
    "    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\\pm$ 3 standard deviations\n",
    "    - Truncation based on inter-quartile range: replace all variable values with $Median \\pm 3s, \\text{ where s }= IQR/(2 \\times 0.6745)$\n",
    "    - Truncation using a sigmoid\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "### Input data\n",
    "\n",
    "There are multiple types of numerical data\n",
    "\n",
    "- Continuous, categorical, integer\n",
    "\n",
    "In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$\n",
    "\n",
    "### Target Variables\n",
    "\n",
    "The core of *supervised learning* is that there is a variable $y$ representing the outcome\n",
    "\n",
    "- Continuous $y$ is a supervised regression problem\n",
    "- Discrete $y$ is a supervised classification problem\n",
    "- Binary $y$ is the easiest version of a supervised classification problem\n",
    "\n",
    "If no set $Y$ outcomes exist, then the problem is *unsupervised*\n",
    "\n",
    "## Defining a model and losses\n",
    "\n",
    "A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)\n",
    "\n",
    "**Mean-square error**: $L(Y,\\hat{Y}) = \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Appropriate for regression analysis\n",
    "\n",
    "**Cross entropy**: When you have no understanding, then entropy (chaos) is at its worst. Each additional piece of information is worth less than the one before.\n",
    "\n",
    "We start with one hot vector $y_i$ where it is 1 if in the correct class and 0 otherwise. There are a total of $k$ classes\n",
    "\n",
    "Eg, If you have 3 classes with probabilities $P_A, P_B, P_C$ with a response vector $y_i = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $f(x_i) = (P_a, P_b, P_c)$,\n",
    "\n",
    "- You can isolate the probabilities by setting $(P_a^{y_a} \\times P_b^{y_b} \\times P_c^{y_c})$\n",
    "- Thus, the loss function is: $L(y_i, \\hat{y}_i) = -\\sum_k y_i^k log\\hat{(y_i^k)}$\n",
    "    - If $P_n$ is 0, then we accept that $0log(0) = 0$\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = -y_ilog(\\hat{y}_i) - (1-y_i)log(1-\\hat{y}_i)$ for a binary version\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = - \\sum_k y_i^k log\\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors\n",
    "\n",
    "### Regression functions\n",
    "\n",
    "The \"best\" model is the one that minimizes the expected error over the full distribution of the set of input and output\n",
    "\n",
    "The problem, mathematically, becomes $min_\\beta E[L(Y,f(x|\\beta))]$\n",
    "\n",
    "**Linear regression example**\n",
    "\n",
    "For the whole sample:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "We can then take the least-squares to find the model for our $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "\"Given a distribution with parameters $\\theta$, how likely are my data?\"\n",
    "\n",
    "We assume that:\n",
    "\n",
    "- Assume points are independent\n",
    "- Can be used with any distribution\n",
    "\n",
    "## Properties of MLE\n",
    "\n",
    "- Functional invariance\n",
    "    - $y=f(\\theta) \\rightarrow \\hat y=f(\\hat \\theta)$\n",
    "- Asymptotic properties\n",
    "    - Estimate is asymptotically unbiased\n",
    "    - Estimate is asymptotically efficient (the best model)\n",
    "    - Estimate is asymptotically normall distributed\n",
    "- If from the normal distribution, the variance MLE is the biased sample variance (dividing by n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "With logistic regression, we can identify how every variable impacts the model.\n",
    "\n",
    "### Classification and regression\n",
    "\n",
    "When you use least-squares, there is no guarantee that your response is between 0 and 1. Also, your target must be normally distributed\n",
    "\n",
    "Logistic regression eliminates ambiguity by pushing middle cases to 0 or 1.\n",
    "    - Intuitively, the odds of an event grow exponentially\n",
    "\n",
    "MLE for logistic regression is **supervised**, so it should have a target. Cases in the sample are independent and identically distributed. iid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "We can compare supervised models using test data. The model with better generalizable characteristics will be better. \n",
    "\n",
    "- we can use the mse, rmse, mae, and the mean relative error as performance measures\n",
    "\n",
    "## Training, validation, and test set\n",
    "\n",
    "1. training set is to calibrate the parameters\n",
    "2. validation set: part of training, but used to make decisions of model construction\n",
    "    - Deciding what variables\n",
    "    - Deciding when to stop training\n",
    "3. test set: **Only** used to calculate final metrics. No decision should be made on the test set\n",
    "\n",
    "## Misuse of the test set\n",
    "\n",
    "- Using it to help build your model, using it to remove variables and calibrate the model\n",
    "    - Leads to overly optimistic models\n",
    "- To do with test set:\n",
    "    1. Split at the very beginning\n",
    "    2. Only decision should be model selection\n",
    "    3. If you need to go back to a previous step, resplit training/validation/test set\n",
    "        - Usually not feasible and expensive, but can have serious consequences if leakage occurs\n",
    "- **MUST replace test set null values with train set median/mean/parameter**, or else there **WILL BE LEAKAGE**\n",
    "\n",
    "## Conditional vs expected test error\n",
    "\n",
    "The **conditional test error** calculates the expectation over different test sets, given a specific training data set. Specific to a given model over a population. \n",
    "\n",
    "- Typically looked at more because it is a fixed model\n",
    "\n",
    "The **expected test error** is taken over both training and test sets, is an average over different model configurations\n",
    "\n",
    "# Decomposing prediction error\n",
    "\n",
    "1. Irreducible error: variability around the true relationship between $x$ and $y$\n",
    "    - This is the best test error we can expect\n",
    "2. Bias: systematic difference of the best fitted model from the true relationship\n",
    "    - $E[\\hat f(x_i)] - f(x_i)$\n",
    "    - Occurs when the function is not the same class of functions fitted\n",
    "    - **Universal approximation property**: some models can approximate any function with enough data: neural networks and tree ensembles\n",
    "3. Variance: variance around the average fit\n",
    "    - $E[\\hat f(x_i) - E[\\hat f(x_i)]]^2$\n",
    "    - If the model is too complex, variance will skyrocket\n",
    "\n",
    "# Bias-variance trade-off\n",
    "\n",
    "We will see that although a more complex model is better for training data, it will not be generalizable for other data sets\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "This is when you have made training error near 0 and thus test error increases a lot\n",
    "\n",
    "Your model should be the best for the data you have and the problem you are trying to solve\n",
    "\n",
    "## Size of the test set\n",
    "\n",
    "- Big enough to detect a difference in test error of interest\n",
    "- Small enough to leave enough data for model fitting\n",
    "    - Rule of thumb: 30% of full data should be allocated to the test data\n",
    "\n",
    "# Measurement in practice\n",
    "\n",
    "1. Performance: how well does the estimated model perform with new observations?\n",
    "2. Decide on how to split the data up: \n",
    "    - Split sample\n",
    "    - N-fold cross validation\n",
    "    - Single sample\n",
    "    - Bootstrapping\n",
    "3. Decide on the performance measure to use\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "### Split sample method\n",
    "\n",
    "- For large data sets with more than 1000 obs, more than 50 groups is sufficient\n",
    "- Distribution of classes should be the same in training and test sets\n",
    "- We need STRICT separation between test and training data\n",
    "\n",
    "### N-fold cross-validation\n",
    "\n",
    "- Typically for smaller data sets under 1000 observations\n",
    "\n",
    "1. Split data into 'N' folds\n",
    "2. Train on 'N-1' folds and test on the remaining fold\n",
    "3. Repeat, where the test set changes every time, thus repeating 'N' times\n",
    "\n",
    "## Out of sample, out of time, out of universe quantitative validation\n",
    "\n",
    "1. Out of sample: test and training data is mixed on one plane\n",
    "    - When mixed with out of time, the data is taken at a different time, so separate\n",
    "2. Out of universe: training and test sets are separated, parallel across time\n",
    "3. Out of time: test set is taken after training or vice versa\n",
    "\n",
    "## Classic performance measures\n",
    "\n",
    "### confusion matrix\n",
    "\n",
    "- can calculate things like sensitivity, specificity, PPV, NPV\n",
    "\n",
    "### ROC curve\n",
    "\n",
    "- AUC larger is better\n",
    "- For multiple classes\n",
    "    - One vs all approach: calculate overall AUC as the weighted average of individual 'n' AUCs\n",
    "    - One vs one approach: calculate nC2 AUCs, then take the average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "diabetes.drop(diabetes[ (diabetes['BMI'] == 0) & (diabetes['BloodPressure'] == 0)].index, inplace=True) # Cannot have 0 BMI, ones with 0 blood pressure do not have useful information\n",
    "diabetes['Glucose'] = diabetes['Glucose'].replace(0,diabetes['Glucose'].median()) # Cannot have 0 Glucose, rows still have other information\n",
    "diabetes['BloodPressure'] = diabetes['BloodPressure'].replace(0,diabetes['BloodPressure'].median()) # Cannot have 0 blood pressure\n",
    "diabetes['BMI'] = diabetes['BMI'].replace(0,diabetes['BMI'].median()) # Cannot have 0 BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posrate = diabetes.Outcome.mean() # 1-baseline accuracy\n",
    "baselineacc = 1-posrate\n",
    "print(f\"The baseline accuracy of the data is {baselineacc}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(diabetes.drop('Outcome',axis='columns'), # splitting into train and test based on the 'outcome' column\n",
    "                                                diabetes['Outcome'], # what is considered the response variable\n",
    "                                                test_size=0.3, # percentage of observations dedicated o the test set\n",
    "                                                random_state=42,  # random state that ensures your results are replicable\n",
    "                                                stratify=diabetes['Outcome']) # stratify by the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_transform = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "                        'Insulin', 'BMI', 'Age', 'DiabetesPedigreeFunction'] # sets the names of the columns we need to use\n",
    "\n",
    "transform_columns = ColumnTransformer([('scaler', StandardScaler(), columns_to_transform)], # scales the columns such that they can all be used in our pipeline\n",
    "                                      remainder='passthrough', # for columns that don't need to be transformed, they are left in our set of columns rather than dropped\n",
    "                                      verbose_feature_names_out=False) # are not given a prefix\n",
    "\n",
    "logit_pipe = Pipeline([ # trains a pipeline to fit a logistic regression model\n",
    "    ('scaler', transform_columns), \n",
    "    ('logistic_regression', LogisticRegression(solver='lbfgs', # default solver for the logistic regression function\n",
    "                                               penalty = None, # no penalty like ridge or lasso\n",
    "                                               max_iter=1000, # number of iterations\n",
    "                                               verbose=1, # given verbosity since positive\n",
    "                                               random_state=42, # random seed\n",
    "                                               n_jobs=-1, # use all processors\n",
    "                                               class_weight='balanced')) # adjusts weights so they are inversely proportional to the frequency\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "\n",
    "logit_pipe.fit(Xtrain, ytrain) # give the pipeline data to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, logit_pipe.predict(Xtest))) # assesses the accuracy of the trained model, using the test set\n",
    "accuracy = logit_pipe.score(Xtest, ytest.to_numpy().ravel()) # also another way of taking the accuracy of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = logit_pipe.predict_proba(Xtest) # the model values for the logistic regression\n",
    "fpr, tpr, thresholds = roc_curve(y_true=ytest, y_score=y_test_prediction[:,1]) # creating the roc curve using the true values vs the model's values\n",
    "auc = np.round(roc_auc_score(y_true=ytest, y_score=y_test_prediction[:,1]), # calculating AUC, again using the true vs predicted values\n",
    "               decimals = 3)\n",
    "confusion_matrix_log = confusion_matrix(ytest, logit_pipe.predict(Xtest)) # creating a confusion matrix using a function\n",
    "sensitivity = confusion_matrix_log[1,1] / (confusion_matrix_log[1,1] + confusion_matrix_log[1,0]) # calculating sensitivity using the confusion matrix\n",
    "specificity = confusion_matrix_log[0,0] / (confusion_matrix_log[0,0] + confusion_matrix_log[0,1]) # calculating specificity using the confusion matrix\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(sensitivity, label=f\"Sensitivity = {sensitivity:.3f}\")\n",
    "plt.plot(specificity, label=f\"Specificity = {specificity:.3f}\")\n",
    "plt.plot(fpr,tpr,label=f\"AUC = {auc:.3f}\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC Curve of Logistic Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty management\n",
    "\n",
    "## Estimation and sampling distribution\n",
    "\n",
    "Parameters characterize the population we want to study, like expectations or values that describe the relationship between input and output\n",
    "\n",
    "- Eg, mean, median, slope\n",
    "\n",
    "Sampling distributions are the distribution of an estimator\n",
    "\n",
    "- How good is the estimate of the population parameter?\n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "For a random variable with a normal distribution, the sample and the population have different variance, thus when we use a confidence interval, we use the sampling standard error. \n",
    "\n",
    "A confidence interval states that for every $\\alpha$%, we expect a certain number out of 100 iterations to contain our true mean, variance, median, etc\n",
    "\n",
    "- Variances however require a chi-squared distribution\n",
    "- Confidence intervals do not work with any ranked measures (AUC, ROC curve)\n",
    "\n",
    "## The bootstrap\n",
    "\n",
    "1. Take a sample of the population\n",
    "2. Resample with replacement an equal sized sample from your original sample. \n",
    "3. Your bootstrap statistics will differ each time\n",
    "    - For a statistic $\\theta$, the distribution of the difference between the original sample statistic and your bootstrapped statistics is asymptotically equal to the real difference between the population statistic and your original sample statistic\n",
    "        - $\\delta_i^\\star = \\theta^\\star - \\theta_i^\\star$\n",
    "    - We can estimate the confidence intervals using the quantiles of $\\delta$\n",
    "\n",
    "The bootstrap is a universal technique to obtain confidence intervals and can be applied to any statistics. The confidence intervals are asymptotically correct and the bootstrap does not make assumptions about the underlying distribution\n",
    "\n",
    "Make sure to save checkpoints and your model outputs so you don't have to reload your training each time.\n",
    "\n",
    "- Can be very unstable when the population is small\n",
    "- Takes time to compute\n",
    "- Applicable when measure is not normally distributed and for complex statistics\n",
    "\n",
    "## Parameter uncertainty\n",
    "\n",
    "Calculating confidence intervals of performance measures:\n",
    "\n",
    "1. Take a trained model\n",
    "2. calculate the bootstrap interval over samples with repetitions of the test set\n",
    "3. Take the confidence interval of that specific model's test distribution, use the center as the original test set's estimate\n",
    "\n",
    "calculating confidence intervals of parameter estimates: using bootstrap to calculate regression parameters\n",
    "\n",
    "1. Take a dataset and calculate the bootstrapped samples from one original sample\n",
    "2. Train the full pipeline over the bootstrapped sample\n",
    "    - splitting train and test sets\n",
    "    - Normalize data\n",
    "    - Train the model\n",
    "    - calculate the performance\n",
    "3. calculate the confidence interval for the parameters using the original training parameter estimate\n",
    "\n",
    "## Bootstrap vs cross-validation\n",
    "\n",
    "- Bootstrap is more precise for most cases\n",
    "- If there are less cases than variables, then cross-validation is more robust\n",
    "- At minimum, run 10 by 10 CV, but better is to run 100 by 100\n",
    "    - CV is cheaper than bootstrap\n",
    "- Cross-validation is to tune hyperparameters and select the best model\n",
    "\n",
    "## Prediction uncertainty\n",
    "\n",
    "How certain can we be with a point estimate? We can calculate a confidence interval based on each bootstrap estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob = logit_pipe.predict_proba(Xtest)[:, 1] # Get the predicted probabilities of the test data\n",
    "\n",
    "ypred = logit_pipe.predict(Xtest) # Get the predicted classes of the test data\n",
    "\n",
    "n_bootstraps = 1000 # set the bootstraps amounts\n",
    "bootstrapped_accuracy = np.zeros(n_bootstraps)\n",
    "bootstrapped_auc = np.zeros(n_bootstraps)\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    idx = np.random.choice(len(ytest), len(ytest), replace=True) # randomly selecting the a set with the length of the test set from the test set with replacement\n",
    "\n",
    "    bootstrapped_accuracy[i] = accuracy_score(ytest.to_pandas().iloc[idx], ypred[idx])  # Get the accuracy of the bootstrap sample\n",
    "\n",
    "    bootstrapped_auc[i] = roc_auc_score(ytest.to_pandas().iloc[idx], yprob[idx]) # Get the AUC of the bootstrap sample\n",
    "    \n",
    "# Get the differences between the bootstrapped values and the original values\n",
    "accuracy_diff = bootstrapped_accuracy - accuracy\n",
    "auc_diff = bootstrapped_auc - auc\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy and AUC\n",
    "accuracy_ci = np.percentile(accuracy_diff, [2.5, 97.5])\n",
    "auc_ci = np.percentile(auc_diff, [2.5, 97.5])\n",
    "\n",
    "# Show the ci bounds for the pipeline developed through bootstrapping\n",
    "print(accuracy_ci)\n",
    "print(auc_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe1 = Pipeline([ # creating another pipeline using logistic regression\n",
    "    ('scaler', transform_columns),\n",
    "    ('logistic_regression', LogisticRegression(solver='lbfgs',\n",
    "                                               penalty = None,\n",
    "                                               max_iter=10000,\n",
    "                                               verbose=0,\n",
    "                                               random_state=0,\n",
    "                                               n_jobs=1,\n",
    "                                               class_weight='balanced'))\n",
    "])\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=100, random_state=0) # creating a random set for cross-validation\n",
    "\n",
    "cv_results = cross_validate(logit_pipe1, # The pipeline to cross-validate\n",
    "                            diabetes.drop('Outcome'), # The features\n",
    "                            diabetes.select('Outcome').to_numpy().ravel(), # The target\n",
    "                            cv=cv, # The cross-validation object we created\n",
    "                            scoring=['accuracy', 'roc_auc'], # The metrics we want to calculate\n",
    "                            return_estimator=True, # Return the estimator for each fold. Useful for calculating parameter uncertainty.\n",
    "                            n_jobs=-1 # Use all available cores\n",
    "                            )\n",
    "\n",
    "logit_pipe1.fit(Xtrain, ytrain.to_numpy().ravel()) # training the pipeline\n",
    "accuracy_scores = cv_results['test_accuracy'] # obtaining the trained pipeline accuracy using the cross-validation function we defined above\n",
    "auc_scores = cv_results['test_roc_auc'] # obtaining the trained pipeline AUC using the cross-validation function we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row and columnar data formats\n",
    "\n",
    "A new data type that was specially formulated for data scientists\n",
    "\n",
    "## How data is stored\n",
    "\n",
    "Data is stored linearly by the computer in a string of zeroes and ones. This is called tabular data in memory\n",
    "\n",
    "We must orient our data to make it make sense to the computer: \n",
    "\n",
    "- Row data storage\n",
    "- Columnar data storage\n",
    "\n",
    "### Row storage (for operational databases)\n",
    "\n",
    "Used in databases, essentially each row from a table all next to each other in one line. Since it is in one row, it is expensive to search all data and replace or delete\n",
    "\n",
    "Eg, Name|Age|Place|Name|Age|Place|Name|Age|Place|\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- each value in the same row are close in space\n",
    "- deleting a row is easy, same as inserting a new row\n",
    "- searching is easy to find all the data within a single row\n",
    "- accessing data is easier in rows (random access)\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Expensive and inefficient for analytics\n",
    "\n",
    "### Columnar storage (Superior data method)\n",
    "\n",
    "Each column from a table is next to each other, sequentially. Is much more efficient for calculating over columns. It also has less data space on your disk\n",
    "\n",
    "Eg, Name|Name|Name|Age|Age|Age|Place|Place|Place\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Single instruction operations are very efficient\n",
    "- Modifying columns is faster\n",
    "- Uncompressed data is more efficient\n",
    "- Compressed data is more efficient\n",
    "- Allows for sinlge instruction, multiple data operations of contiguous column data\n",
    "- Data for each column is stored contiguously in memory\n",
    "\n",
    "## Columnar vs row data formats\n",
    "\n",
    "For operational databases, we should stick to **row formats**\n",
    "\n",
    "- Random access is needed\n",
    "- We can delete and edit with ease\n",
    "\n",
    "For data science pipelines, we should use **columnar data**\n",
    "\n",
    "- We normally do operations over rows\n",
    "- Space is an issue\n",
    "- We rarely delete or add data\n",
    "\n",
    "## Spark / Arrow / DuckDB\n",
    "\n",
    "**Apache Parquet** (in disk) and **Apache Arrow** (in memory) are two softwares that store data as columnar\n",
    "\n",
    "Implementations\n",
    "\n",
    "- DuckDB: traditional databases with columnar storage, optimized for data warehousing and storage\n",
    "- Polars: Library for data manipulation, well-structured API that is expressive and easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and regularization\n",
    "\n",
    "## Approaches to feature selection\n",
    "\n",
    "1. Subset selection: identifying a subset of predictors that we believe is related to the response\n",
    "2. Shrinkage/regularization: fitting a model with all predictors but shrinking the coefficients\n",
    "3. Dimension reduction\n",
    "\n",
    "### Sequential selection: greedy searches\n",
    "\n",
    "1. forward selection: start from 0 predictors, then add one each time until the next one we add doesn't help us\n",
    "    - May miss the best model since it doesn't consider the interactions between the predictors\n",
    "2. backward selection: start with all, take off predictors until we cannot feasibly do so anymore\n",
    "    - Usually ends up with more predictors than forward selection\n",
    "3. Stepwise selection: evaluates whether we should add or subtract predictors at each step\n",
    "\n",
    "Greedy searches do not lead to a globally optimal solution\n",
    "\n",
    "### Regularization/shrinkage\n",
    "\n",
    "These approaches work on penalties when the model uses more variables than necessary\n",
    "\n",
    "We define the loss as:\n",
    "\n",
    "$$L(x,y| f) = L_{Bias}(x,y|f) + \\lambda L_{variance}(x,y|f)$$\n",
    "\n",
    "$lambda$ is a **hyperparameter** that can be set to define any configurable part of a model's learning process\n",
    "\n",
    "- It changes the function itself, the outcome, and capacity of the model\n",
    "- It is a modeler's choice and depends on the problem\n",
    "- The $\\lambda$ must be tuned using cross-validation\n",
    "- As $\\lambda$ increases, the variance decreases and the bias increases\n",
    "\n",
    "#### Ridge regression (L2 norm)\n",
    "\n",
    "Imposes a squared loss: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V \\beta_k^2$\n",
    "\n",
    "- variables must be standardized to ensure the model converges to a solution\n",
    "- Does not select variables, but shrinks them to near 0 instead. \n",
    "- Eliminates correlations between variables\n",
    "\n",
    "#### Lasso regression (L1 norm)\n",
    "\n",
    "Imposes an absolute value penalty: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V |\\beta_k|$\n",
    "\n",
    "- can eliminate variables - variable selection\n",
    "- Needs cleaning\n",
    "- Coefficients go to 0 very fast\n",
    "- does not allow for correlation between variables\n",
    "- variables need to be normalized, and it is much more sensitive method to de-normalized variables\n",
    "- good for industries with lots of variables and not that much data\n",
    "\n",
    "#### Elasticnet\n",
    "\n",
    "Imposes an average between ridge and lasso: $\\lambda L_{variance}(x,y|f) = \\lambda \\bigg( \\alpha\\sum_{k=1}^V |\\beta_k| + \\frac{1-\\alpha}{2} \\sum_{k=1}^V \\beta_k^2\\bigg)$\n",
    "\n",
    "- $\\alpha$ is a balance parameter between the weight given to LASSO vs ridge\n",
    "- Typically want to give more weight to LASSO than ridge, unless data is highly correlated\n",
    "- Make sure to read the documentation of elastic net in python since it is not written as it is above\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
