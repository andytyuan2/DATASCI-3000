{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework final\n",
    "\n",
    "- Andy Yuan\n",
    "- Aidan Dignam\n",
    "- Amelia Walker\n",
    "- Owen Stevenson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Tree-based models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, train_test_split, StratifiedKFold, cross_val_score, \n",
    "    cross_validate, RepeatedKFold\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    silhouette_samples, silhouette_score, confusion_matrix, \n",
    "    roc_curve, roc_auc_score, classification_report, accuracy_score, r2_score\n",
    ")\n",
    "\n",
    "# Preprocessing and feature selection\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# Shap\n",
    "import shap\n",
    "shap.initjs() # Import Java engine.\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistical and optimization tools\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import zscore\n",
    "from scipy.special import factorial\n",
    "import scipy.optimize as so\n",
    "\n",
    "# Text processing\n",
    "import sklearn.feature_extraction.text as sktext\n",
    "import re\n",
    "\n",
    "# Dimensionality reduction\n",
    "import umap\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_csv(\"../Coursework Data/Household data.csv\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Data clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data.to_pandas()), columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterer and elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clusterer\n",
    "\n",
    "KClusterer = KMeans(n_clusters=3, # initialized with 3 for simplicity\n",
    "                   verbose=0,\n",
    "                   random_state=2025) # Name of operator and cluster number\n",
    "\n",
    "# Elbow visualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(KClusterer,\n",
    "                              k=(2,20),\n",
    "                              locate_elbow=True,\n",
    "                              timings=False)\n",
    "\n",
    "visualizer.fit(data_scaled)\n",
    "visualizer.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette visualizer\n",
    "\n",
    "fig, axes = plt.subplots(15, 3, figsize=(15,10))\n",
    "axes = axes.flatten()\n",
    "sil_scores = {}\n",
    "\n",
    "for i in range(2,20):\n",
    "    Kmeansclusterer = KMeans(n_clusters=i,\n",
    "                             verbose=0,\n",
    "                             random_state=2025\n",
    "                                            )\n",
    "    cluster_labels = Kmeansclusterer.fit_predict(data_scaled)\n",
    "    sil_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    sil_scores[f'{i} clusters'] = sil_avg\n",
    "    \n",
    "    visualizer = SilhouetteVisualizer(Kmeansclusterer, colors='yellowbrick', ax=axes[i-1])\n",
    "    visualizer.fit(data_scaled)  # Fit only the scaled data, not cluster labels\n",
    "    axes[i-1].set_title(f\"{i} Clusters - Silhouette: {sil_avg:.2f}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette Scores:\", sil_scores)\n",
    "max_values = heapq.nlargest(3, sil_scores.values())\n",
    "max_keys = [key for key, value in sil_scores.items() if value in max_values]\n",
    "print(\"The greatest silhouette scores are from \", max_keys, \"with scores of\", max_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying k-means according to best cluster number (data not scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KClusterer = KMeans(n_clusters=3, # put in optimal cluster amount\n",
    "                   verbose=0,\n",
    "                   random_state=2025) # Name of operator and cluster number\n",
    "\n",
    "data_np = data.to_numpy() # not the scaled version\n",
    "cluster_labels = KClusterer.fit_predict(data_np)\n",
    "data_cluster_table = data.with_columns(pl.Series(\"cluster_label\", cluster_labels))\n",
    "\n",
    "cluster_avg = data_cluster_table.group_by(\"cluster_label\").agg([\n",
    "    pl.col(column).mean().alias(f\"{column}_mean\") for column in data_cluster_table.columns if column != \"cluster_label\"\n",
    "])\n",
    "print(cluster_avg.sort('cluster_label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Linear dimensionality reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf-idf transformer\n",
    "TfIDFTransformer = sktext.TfidfVectorizer(strip_accents='unicode', # Eliminate accents and special characters\n",
    "                      stop_words='english', # Eliminates stop words.\n",
    "                      min_df = 0.01, # Eliminate words that do not appear in more than 5% of texts\n",
    "                      max_df = 0.95, # Eliminate words that appear in more than 95% of texts\n",
    "                      sublinear_tf=True # Use sublinear weights (softplus)\n",
    "                      )\n",
    "TfIDFdata = TfIDFTransformer.fit_transform(data[\"text columns\"])\n",
    "word_index = TfIDFTransformer.get_feature_names_out()\n",
    "len(word_index)\n",
    "\n",
    "## PCA\n",
    "nPCA = PCA(n_components=100)\n",
    "nPCA.fit(np.asarray(TfIDFdata.todense()))\n",
    "total_variance = np.sum(nPCA.explained_variance_) * 100\n",
    "print('The total explained variance of the first %i components is %.3f percent' % (nPCA.n_components_, total_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Plot the data of the first two PCs in a scatterplot (scaled data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = nPCA.transform(np.asarray(TfIDFdata.todense()))  # Transform TF-IDF data\n",
    "pc1 = pca_components[:, 0]  # First principal component\n",
    "pc2 = pca_components[:, 1]  # Second principal component\n",
    "pc3 = pca_components[:, 2]  # Second principal component\n",
    "\n",
    "cluster_labels_scaled = KClusterer.fit_predict(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(\n",
    "    pc1, \n",
    "    pc2, \n",
    "    c=cluster_labels_scaled, \n",
    "    cmap='viridis',  # You can change to 'plasma', 'rainbow', etc.\n",
    "    alpha=0.6,       # Slightly transparent points\n",
    "    edgecolors='w',  # White edges for better visibility\n",
    "    s=50            # Point size\n",
    ")\n",
    "\n",
    "plt.xlabel('Principal Component 1 (PC1)', fontsize=12)\n",
    "plt.ylabel('Principal Component 2 (PC2)', fontsize=12)\n",
    "plt.title('PCA Scatterplot Colored by Cluster', fontsize=14)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Cluster Label', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Calculate average value of first three components, put into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame({\n",
    "    'PC1': pc1,\n",
    "    'PC2': pc2,\n",
    "    'PC3': pc3,\n",
    "    'Cluster': cluster_labels_scaled  # From KClusterer.fit_predict(data_scaled)\n",
    "})\n",
    "\n",
    "cluster_means = pca_df.groupby('Cluster').mean().reset_index()\n",
    "\n",
    "def name_cluster(row):\n",
    "    max_pc = max(['PC1', 'PC2', 'PC3'], key=lambda x: abs(row[x]))\n",
    "    if max_pc == 'PC1':\n",
    "        return \"High PC1 (Topic A)\" if row['PC1'] > 0 else \"Low PC1 (Topic B)\"\n",
    "    elif max_pc == 'PC2':\n",
    "        return \"High PC2 (Topic C)\" if row['PC2'] > 0 else \"Low PC2 (Topic D)\"\n",
    "    else:\n",
    "        return \"High PC3 (Topic E)\" if row['PC3'] > 0 else \"Low PC3 (Topic F)\"\n",
    "\n",
    "cluster_means['Cluster_Name'] = cluster_means.apply(name_cluster, axis=1)\n",
    "print(cluster_means[['Cluster', 'Cluster_Name', 'PC1', 'PC2', 'PC3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters to use:\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 15, 30, 50],\n",
    "    'min_dist': [0.01, 0.1, 0.5],\n",
    "    'metric': ['euclidean', 'cosine']\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "best_embedding = None\n",
    "\n",
    "# Grid search (small-scale for demonstration)\n",
    "for n in param_grid['n_neighbors']:\n",
    "    for d in param_grid['min_dist']:\n",
    "        for m in param_grid['metric']:\n",
    "            reducer = umap.UMAP(n_neighbors=n, min_dist=d, metric=m, random_state=2025)\n",
    "            embedding = reducer.fit_transform(np.asarray(TfIDFdata.todense()))\n",
    "            score = silhouette_score(embedding, cluster_labels)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'n_neighbors': n, 'min_dist': d, 'metric': m}\n",
    "                best_embedding = embedding\n",
    "\n",
    "print(f\"Best parameters: {best_params}, Silhouette score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=10,              # Number of neareast neighbours to use. (set to best)\n",
    "                    n_components=2,              # Number of components. UMAP is robust to larger values\n",
    "                    metric='cosine',             # Metric to use. (set to best)\n",
    "                    n_epochs=1000,               # Iterations. Set to convergence. None implies either 200 or 500.\n",
    "                    min_dist=0.1,                # Minimum distance embedded points. Smaller makes clumps, larger, sparseness. (set to best)\n",
    "                    spread=1.0,                  # Scale to combine with min_dist\n",
    "                    low_memory=False,             # Run slower, but with less memory.\n",
    "                    n_jobs=-1,                   # Cores to use\n",
    "                    verbose=0                 # Verbosity\n",
    "                   )\n",
    "UMAP_embedding = reducer.fit_transform(TfIDFdata)\n",
    "\n",
    "sns.scatterplot(x=UMAP_embedding[:, 0], y=UMAP_embedding[:, 1], hue=cluster_labels_scaled)\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"UMAP of data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Regularized elastic-net linear regression from data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
