{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework final\n",
    "\n",
    "- Andy Yuan\n",
    "- Aidan Dignam\n",
    "- Amelia Walker\n",
    "- Owen Stevenson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Tree-based models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, train_test_split, StratifiedKFold, cross_val_score, \n",
    "    cross_validate, RepeatedKFold\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    silhouette_samples, silhouette_score, confusion_matrix, \n",
    "    roc_curve, roc_auc_score, classification_report, accuracy_score, r2_score\n",
    ")\n",
    "\n",
    "# Preprocessing and feature selection\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# Shap\n",
    "import shap\n",
    "shap.initjs() # Import Java engine.\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistical and optimization tools\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import zscore\n",
    "from scipy.special import factorial\n",
    "import scipy.optimize as so\n",
    "\n",
    "# Text processing\n",
    "import sklearn.feature_extraction.text as sktext\n",
    "import re\n",
    "\n",
    "# Dimensionality reduction\n",
    "import umap\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "from itertools import chain, combinations\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_csv(\"Household data.csv\")\n",
    "data.describe()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data.to_pandas()), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k- means cluster\n",
    "\n",
    "KClusterer = KMeans(n_clusters=3,\n",
    "                   verbose=0,\n",
    "                   random_state=2025) # Name of operator and cluster number\n",
    "\n",
    "mall2 = data_scaled.copy()\n",
    "mall2['cluster_label'] = KClusterer.fit_predict(data_scaled)\n",
    "\n",
    "\n",
    "sns.pairplot(vars=data_scaled.columns, # Variable names\n",
    "             hue='cluster_label',        # How to colour the points. Use cluster labels\n",
    "             markers=['X','o','^'],      # Differentiate markers\n",
    "             data=mall2,                     # What data to use\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "\n",
    "## td-idf transformer\n",
    "TfIDFTransformer = sktext.TfidfVectorizer(strip_accents='unicode', # Eliminate accents and special characters\n",
    "                      stop_words='english', # Eliminates stop words.\n",
    "                      min_df = 0.01, # Eliminate words that do not appear in more than 5% of texts\n",
    "                      max_df = 0.95, # Eliminate words that appear in more than 95% of texts\n",
    "                      sublinear_tf=True # Use sublinear weights (softplus)\n",
    "                      )\n",
    "TfIDFdata = TfIDFTransformer.fit_transform(data[\"text column\"])\n",
    "word_index = TfIDFTransformer.get_feature_names_out()\n",
    "len(word_index)\n",
    "\n",
    "## PCA\n",
    "nPCA = PCA(n_components=100)\n",
    "nPCA.fit(np.asarray(TfIDFyelp.todense()))\n",
    "total_variance = np.sum(nPCA.explained_variance_) * 100\n",
    "print('The total explained variance of the first %i components is %.3f percent' % (nPCA.n_components_, total_variance))\n",
    "\n",
    "## part 1 c i and ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=10,              # Number of neareast neighbours to use.\n",
    "                    n_components=2,              # Number of components. UMAP is robust to larger values\n",
    "                    metric='cosine',             # Metric to use.\n",
    "                    n_epochs=1000,               # Iterations. Set to convergence. None implies either 200 or 500.\n",
    "                    min_dist=0.1,                # Minimum distance embedded points. Smaller makes clumps, larger, sparseness.\n",
    "                    spread=1.0,                  # Scale to combine with min_dist\n",
    "                    low_memory=False,             # Run slower, but with less memory.\n",
    "                    n_jobs=-1,                   # Cores to use\n",
    "                    verbose=0                 # Verbosity\n",
    "                   )\n",
    "UMAP_embedding = reducer.fit_transform(TfIDFdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
