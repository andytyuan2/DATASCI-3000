{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment 5: Logistic Regression with Variable Selection\n","\n","## Follow These Steps Before Submitting\n","Once you are finished, ensure to complete the following steps.\n","\n","1.  Restart your kernel by clicking **'Runtime' > 'Restart session and run all'**.\n","\n","2.  Fix any errors which result from this.\n","\n","3.  Repeat steps 1. and 2. until your notebook runs without errors.\n","\n","4.  Submit your completed notebook to OWL by the deadline."],"metadata":{"id":"m9l0b9ZnSgPd"}},{"cell_type":"markdown","source":["# Dataset\n","\n","In this assignment, you will work on a dataset taken from USGS(U.S Geological Survey). This dataset contains earthquake data with a magnitude of 4.5+ and an \"alert\" warning level, recorded between 1976 and 2025. Below is an explanation of the columns included in the dataset:\n","\n","- **`id`**: A unique identifier for the earthquake event.\n","- **`time`**: The timestamp indicating when the earthquake or event occurred, including the date and time in UTC format.\n","- **`latitude`**: The geographical latitude of the earthquake's epicenter, measured in degrees.\n","- **`longitude`**: The geographical longitude of the earthquake's epicenter, measured in degrees.\n","- **`depth`**: The depth at which the earthquake occurred, typically measured in kilometers below the Earth's surface.\n","- **`mag`**: The magnitude of the earthquake, representing the energy released by the seismic event. In this case, a value of 8.6 indicates a very large earthquake.\n","- **`gap`**: The azimuthal gap, which refers to the angular distance between the two most distant seismic stations that recorded the earthquake. A smaller gap typically indicates better global coverage.\n","- **`dmin`**: The minimum distance between the earthquake's epicenter and the nearest seismic station, measured in degrees.\n","- **`rms`**: The root mean square of the amplitude of the seismic waves, representing the strength of the seismic signal.\n","- **`horizontalError`**: The error associated with the latitude and longitude coordinates of the epicenter, typically measured in kilometers.\n","- **`depthError`**: The error associated with the depth measurement of the earthquake, typically measured in kilometers.\n","- **`magError`**: The error associated with the magnitude measurement of the earthquake, representing the uncertainty in the reported magnitude.\n","- **`magNst`**: The number of stations that contributed to the magnitude estimation.\n","- **`Alert` (target)** The alert level issued for the earthquake, whether 'Severe' or 'Non-Severe'.\n","\n","The goal is to train a model for predicting the **`Alert`** which indicates the severity of the earthquake.\n","\n"],"metadata":{"id":"LJCGYX0iRh9j"}},{"cell_type":"code","source":["# Standard imports\n","import numpy as np\n","from itertools import chain, combinations\n","\n","# Data manipulation\n","import pandas as pd\n","import polars as pl\n","\n","# Sklearn imports\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_curve, roc_auc_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_selection import SequentialFeatureSelector\n","\n","# Plotting packages\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"],"metadata":{"id":"zoXM0sRHQne_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download the data\n","!gdown https://drive.google.com/uc?id=1yL84FMQrfHC_cQsa_V3KTcRAJS0k4DhY"],"metadata":{"id":"dIBHhqMZUx3R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 1: Data Preprocessing"],"metadata":{"id":"l8G0CQ-bXMMM"}},{"cell_type":"markdown","source":["## Question 1.1: Load data\n","\n","(1) Read the **`earthquakes.parquet`** file as a **`polars.DataFrame`** and show its descriptive statistics.\n","\n","(2) Drop column **`id`** and **`time`** and display the first 5 rows of the dataframe.\n","\n","Since **`id`** is unique for each earthquake event that does not contain any predictive information and **`time`** is not directly informative for predicting earthquake severity unless you extract relevant features such as time of day, seasonality, etc."],"metadata":{"id":"UMaaQ19-Xd1d"}},{"cell_type":"code","source":["# (1) YOUR CODE HERE\n"],"metadata":{"id":"B6_VKNYOWLnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (2) YOUR CODE HERE\n"],"metadata":{"id":"6VQganpcnIEZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 1.2: Handle null values\n","\n","The result of the `null_count` function indicates that some columns contain null values. Fill these null values with the median of the corresponding column and display the first 5 rows of the resulting dataframe."],"metadata":{"id":"Yv9TQIWqoWpY"}},{"cell_type":"code","source":["# df.null_count() # uncomment and run this code"],"metadata":{"id":"5SkBkhO7ouYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"YvX9BKCnovfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 1.3: Explore target distribution\n","\n","Count the number of instances of each severity level of the earthquake in the dataset.\n","\n","Comment on your findings, providing insights into the distribution of different severity levels."],"metadata":{"id":"61faU0aZzRqk"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"8T3pp1W0zRDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Your written answer here)\n"],"metadata":{"id":"ZtiF3a-qzpPy"}},{"cell_type":"markdown","source":["## Question 1.4: Convert target variable\n","\n","Convert **`Alert`** to a binary numerical target:\n","- Replace **`Severe`** with 1.\n","- Replace **`Non-Severe`** with 0.\n","\n","Display the first 5 rows of the resulting dataframe.\n","\n","Hint: If you use the `replace` method, the resulting column will still be of string type. Use `cast` to make it `Float64` after replacement."],"metadata":{"id":"766tNRJzbBFA"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"qjO3iAxvaTpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 1.5: Train test split\n","\n","Split the dataset into training and testing sets:\n","- With **30% testing data** and **70% training data**.\n","- Set the **random state** to **2025**.\n","- Use **stratified splitting** to **maintain the same proportion of each class** in the target variable (**`Alert`**) in both the training and testing sets.\n","\n","Display the descriptive statistics for X_train and X_test."],"metadata":{"id":"mxDVkMQ-hEem"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"DG9n0DVHwJp0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 2: Sequential Feature Selection"],"metadata":{"id":"DkcOJ8fErgu8"}},{"cell_type":"markdown","source":["## Question 2.1: Forward Selection\n","\n","Create a pipeline using [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) to perform **forward** feature selection:\n","- Standardize the data, ensuring each feature has a mean of 0 and a standard deviation of 1.\n","- Use **ROC AUC** as the scoring metric for feature selection. Conduct **5-fold cross-validation** to evaluate the model. Set the **tolerance for stopping** the selection process to **0.001**.\n","- Configure the logistic regression to use the default **`lbfgs`** as solver with **no penalty**. Set the **maximum number of iterations** to **1000**. Use a **balanced** weight that adjust weights inversely proportional to class frequencies in the input data. Set the **random state** to **2025**.\n","\n","Fit the pipeline and report the subset of variables on this method.\n"],"metadata":{"id":"6MKT3NRarkz5"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"I0gfWRgo1P14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2.2: Backward Selection\n","\n","Create a pipeline using [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) to perform **backward** feature selection. Keep all other configurations the same as in the previous question\n","\n","Fit the pipeline and report the subset of variables on this method.\n"],"metadata":{"id":"ACAsAnUdLB_o"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"fnrafwMxK08Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2.3: Compare results & find the best model\n","\n","Compare and discuss the selected subset of variables obtained from both methods used in the previous steps.\n","\n","Perform an **exhaustive search** over all possible subsets of the remaining variables using **5-fold cross-validation** to find the best model. Use the same Logistic Regression configurations as in previous questions.\n","\n","Hint: If you have correctly followed the previous steps, you should have **five remaining variables** to evaluate in the exhaustive search."],"metadata":{"id":"8cSXKQpXMlwm"}},{"cell_type":"markdown","source":["(Your written answer here)\n","\n","Among all 11 variables, forward selection resulted in 5 features, while backward selection selected 10 features. This indicates that backward selection retains more variables, perhaps due to its nature of removing less impactful features instead of building up the model iteratively. Both selection results agree on keeping `['latitude', 'depth', 'mag', 'dmin', 'horizontalError']`. We should then conduct an exhaustive search using the remaining five variables `['longitude', 'gap', 'depthError', 'magError', 'magNst']`."],"metadata":{"id":"fEAnA4JUOgw_"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"Qjjk41LbMlo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2.4: Fit the best model\n","\n","Train a logistic regression model with the best variables selected.\n","\n","Display the model's coefficients and intercept."],"metadata":{"id":"Qe7q3KFzSCOp"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"YdCYh9e3MlPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2.5: Measure model's performance\n","\n","Construct a **95% confidence interval** for both accuracy and AUC using **100 bootstrap resamples** of the test set."],"metadata":{"id":"NkqKda5RV0hZ"}},{"cell_type":"code","source":["np.random.seed(2025) # DO NOT DELETE\n","\n","# YOUR CODE HERE\n"],"metadata":{"id":"u7JoX1dnMFUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 3: Regularization"],"metadata":{"id":"DMFb9isT5HG7"}},{"cell_type":"markdown","source":["## Question 3.1: Ridge penalization\n","\n","Create a pipeline using [`LogisticRegressionCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) to implement a logistic regression with **Ridge penalization**:\n","- Standardize the data, ensuring each feature has a mean of 0 and a standard deviation of 1.\n","- Configure the **`LogisticRegressionCV`** to use **`saga`** as solver. Use the default **`Cs = 10`**. Set the **maximum number of iterations** to **1000**. Use a **balanced** weight that adjust weights inversely proportional to class frequencies in the input data. Set the **random state** to **2025**. Use **5-fold cross-validation**.\n","\n","Fit the pipeline to get the best model.\n","\n","Construct a **95% confidence interval** for both accuracy and AUC using **100 bootstrap resamples** of the test set."],"metadata":{"id":"UN8KyRvX6D_y"}},{"cell_type":"code","source":["np.random.seed(2025) # DO NOT DELETE\n","\n","# YOUR CODE HERE\n"],"metadata":{"id":"QZh1TMq942kd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 3.2: Lasso penalization\n","\n","Create a pipeline using [`LogisticRegressionCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) to implement a logistic regression with **Lasso penalization**:\n","- Standardize the data, ensuring each feature has a mean of 0 and a standard deviation of 1.\n","- Configure the **`LogisticRegressionCV`** to use **`saga`** as solver. Use the default **`Cs = 10`**. Set the **maximum number of iterations** to **1000**. Use a **balanced** weight that adjust weights inversely proportional to class frequencies in the input data. Set the **random state** to **2025**. Use **5-fold cross-validation**.\n","\n","Fit the pipeline to get the best model.\n","\n","Construct a **95% confidence interval** for both accuracy and AUC using **100 bootstrap resamples** of the test set."],"metadata":{"id":"LWwwyAFxEs3O"}},{"cell_type":"code","source":["np.random.seed(2025) # DO NOT DELETE\n","\n","# YOUR CODE HERE\n"],"metadata":{"id":"EOFq1EqUDR5F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 3.3: Ridge vs Lasso\n","\n","Report the coefficients from the best-performing models with Ridge and Lasso penalties.\n","\n","Compare and discuss how the coefficient magnitudes differ between Ridge and Lasso, and explain."],"metadata":{"id":"yB9OrR_SFmUq"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"90aHPsKTE6vc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Your written answer here)\n"],"metadata":{"id":"3fgLpuw3ItWi"}},{"cell_type":"markdown","source":["# Part 4: Overall Comparison\n","\n","Compare the best models obtained using **Sequential Feature Selection**, **Ridge Regularization**, and **Lasso Regularization** by plotting the ROC curve for each model on a single plot. Additionally, include a diagonal reference line representing random classification performance (i.e., an ROC curve with an AUC of 0.5).\n","\n","Provide a brief analysis and comment on your findings (no need to identify the best model)."],"metadata":{"id":"DoMw6CaUMx-B"}},{"cell_type":"code","source":["# YOUR CODE HERE\n"],"metadata":{"id":"VI4pxr-mHj8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Your written answer here)\n"],"metadata":{"id":"rxg6xDcWPzyc"}}]}