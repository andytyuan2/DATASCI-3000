{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this coursework, you will classify houses into **high price** and **low price** categories based on their characteristics using tree-based methods.\n",
    "\n",
    "### **Objectives**\n",
    "- Use the **Ames Housing dataset**, which contains detailed property attributes.\n",
    "- Train and compare **Decision Tree, Random Forest, and XGBoost** classifiers.\n",
    "- Evaluate models using **confusion matrices and ROC-AUC**.\n",
    "- Optimize hyperparameters with **GridSearchCV**.\n",
    "- Interpret model decisions using **SHAP explainability**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from io import BytesIO\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# Shap\n",
    "import shap\n",
    "shap.initjs() # Import Java engine.\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fd965",
   "metadata": {},
   "source": [
    "### **Dataset Description**\n",
    "The dataset used is the **Ames Housing dataset**, which contains detailed information about residential houses in Ames, Iowa. Each row represents a house, and each column describes a characteristic of that house. Let's understand the **15 most important features** in the dataset:\n",
    "\n",
    "1. **OverallQual**: The overall material and finish quality of the house.\n",
    "2. **GrLivArea**: Above ground living area in square feet.\n",
    "3. **GarageCars**: Number of car spaces in the garage.\n",
    "4. **TotalBsmtSF**: Total basement area in square feet.\n",
    "5. **1stFlrSF**: First-floor area in square feet.\n",
    "6. **FullBath**: Number of full bathrooms.\n",
    "7. **TotRmsAbvGrd**: Total number of rooms above ground level (excluding bathrooms).\n",
    "8. **YearBuilt**: Year the house was constructed.\n",
    "9. **Fireplaces**: Number of fireplaces in the house.\n",
    "10. **GarageArea**: Size of the garage in square feet.\n",
    "11. **LotArea**: Total lot size in square feet.\n",
    "12. **KitchenQual**: Kitchen quality rating.\n",
    "13. **BsmtFinSF1**: Finished basement square footage.\n",
    "14. **Neighborhood**: The general location of the property within Ames.\n",
    "15. **MSSubClass**: Identifies the type of dwelling.\n",
    "\n",
    "\n",
    "- **Target Variable:** `SalePrice` (converted into a classification task: High vs. Low price)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d60a93",
   "metadata": {},
   "source": [
    "## **Question 1: Data Preprocessing**\n",
    "Before training your models, you need to clean and preprocess the dataset.\n",
    "\n",
    "1. **Load the data `housing.csv` and display the first 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ames Housing Dataset\n",
    "housing = pl.read_csv(\"housing.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "housing.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9b95a",
   "metadata": {},
   "source": [
    "1. **Convert the `SalePrice` column into a binary classification variable:**\n",
    "   - If the house price is **above the median**, label it as **1 (High price)**.\n",
    "   - If the house price is **below the median**, label it as **0 (Low price)**.\n",
    "   \n",
    "2. **Drop the original `SalePrice` column** after conversion.\n",
    "\n",
    "3. **Encode categorical variables**:\n",
    "   - Some columns contain text (e.g., `Neighborhood`, `House Style`).\n",
    "   - Convert them into numerical values using **Label Encoding**.\n",
    "\n",
    "4. **Standardize numerical features**:\n",
    "   - Scale numerical values to improve model performance.\n",
    "   - Use `StandardScaler()` from `sklearn.preprocessing`.\n",
    "\n",
    "5. **Split the dataset into training and testing sets**:\n",
    "   - Use an **80-20 split** (`train_test_split`).\n",
    "   - Set `random_state=42` for reproducibility.\n",
    "   - Use `stratify=y` to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SalePrice into a binary target variable (1 for high price, 0 for low price)\n",
    "housing = housing.with_columns(\n",
    "    (pl.col(\"SalePrice\") >= housing[\"SalePrice\"].median()).cast(pl.Float64()).alias(\"SalePrice2\")\n",
    ")\n",
    "\n",
    "# Drop original SalePrice column\n",
    "housing = housing.drop(\"SalePrice\")\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = [col for col in housing.columns if housing[col].dtype == pl.Utf8]\n",
    "le = LabelEncoder()\n",
    "encoded_columns = {col: le.fit_transform(housing[col].to_list()) for col in categorical_columns}\n",
    "housing = housing.with_columns([pl.Series(col, encoded_columns[col]) for col in categorical_columns])\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = housing.drop(\"SalePrice2\")\n",
    "X_columns = X.columns\n",
    "Y = housing[\"SalePrice2\"] \n",
    "\n",
    "# Convert X to NumPy before train-test split\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Perform Train-Test Split (with stratification)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,\n",
    "                                                Y,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=42,\n",
    "                                                stratify=Y\n",
    "                                                )\n",
    "\n",
    "# Standardize Numerical Features\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62d29f",
   "metadata": {},
   "source": [
    "## **Question 2: Model Training and Evaluation**\n",
    "You will train three classification models:\n",
    "- **Decision Tree** (`random_state = 42`)\n",
    "- **Random Forest** (`random_state = 42`)\n",
    "\n",
    "1. **Train the models on the training set. Make sure you use the optimal number of trees for the Random Forest.**.\n",
    "2. **Evaluate models using the following metrics**:\n",
    "   - **Confusion Matrix**: Displays True Positives, True Negatives, False Positives, and False Negatives.\n",
    "   - **ROC-AUC Score**: Measures the modelâ€™s ability to distinguish between classes.\n",
    "   - **ROC Curve**: Plots True Positive Rate (TPR) vs. False Positive Rate (FPR).\n",
    "\n",
    "3. **Plot the ROC curves for all models in one figure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids\n",
    "plt.figure(figsize=(10, 15))\n",
    "depths = {\"classifier__max_depth\":np.arange(1,12)}\n",
    "param_grid = {\"classifier__n_estimators\": np.arange(30,100)}\n",
    "\n",
    "# Define base models\n",
    "\n",
    "            # Decision tree base model\n",
    "reg = DecisionTreeClassifier(max_depth=12)\n",
    "clf = Pipeline(steps=[(\"classifier\",reg)])\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            # Random forest base model\n",
    "placement_rf = RandomForestClassifier(n_estimators=30, # Number of trees to train\n",
    "                       criterion='entropy', # How to train the trees. Also supports entropy.\n",
    "                       max_depth=None, # Max depth of the trees. Not necessary to change.\n",
    "                       min_samples_split=2, # Minimum samples to create a split.\n",
    "                       min_samples_leaf=0.001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n",
    "                       min_weight_fraction_leaf=0.0, # Same as above, but uses the class weights.\n",
    "                       max_features='sqrt', # Maximum number of features per split (not tree!) by default is sqrt(vars)\n",
    "                       max_leaf_nodes=None, # Maximum number of nodes.\n",
    "                       min_impurity_decrease=0.0001, # Minimum impurity decrease. This is 10^-3.\n",
    "                       bootstrap=True, # If sample with repetition. For large samples (>100.000) set to false.\n",
    "                       oob_score=True,  # If report accuracy with non-selected cases.\n",
    "                       n_jobs=-1, # Parallel processing. Set to -1 for all cores. Watch your RAM!!\n",
    "                       random_state=42, # Seed\n",
    "                       verbose=0, # If to give info during training. Set to 0 for silent training.\n",
    "                       warm_start=False, # If train over previously trained tree.\n",
    "                       class_weight='balanced', # Balance classes.\n",
    "                                    )\n",
    "rff = Pipeline(steps=[(\"classifier\",placement_rf)])\n",
    "rff.fit(Xtrain, ytrain)\n",
    "# Hyperparameter tuning and model evaluation\n",
    "    # Perform Grid Search to find the best hyperparameters\n",
    "grid_search_dtree = GridSearchCV(clf, depths, cv=5)\n",
    "grid_search_rf = GridSearchCV(rff, param_grid, cv=5)\n",
    "    # Get the best model\n",
    "grid_search_dtree.fit(Xtrain, ytrain) # decision tree\n",
    "grid_search_rf.fit(Xtrain, ytrain) # random forest\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70305252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "dt_pred_class_test = grid_search_dtree.predict(Xtest)\n",
    "dt_probs_test = grid_search_dtree.predict_proba(Xtest)\n",
    "rf_pred_class_test = grid_search_rf.predict(Xtest)\n",
    "rf_probs_test = grid_search_rf.predict_proba(Xtest)\n",
    "    # Evaluation metrics\n",
    "   # Decision tree\n",
    "confusion_matrix_dt = confusion_matrix(y_true = ytest.to_numpy(), y_pred = dt_pred_class_test)\n",
    "confusion_matrix_dt = confusion_matrix_dt.astype('float') / confusion_matrix_dt.sum(axis=1)[:, np.newaxis]\n",
    "df_cm_dt = pd.DataFrame(\n",
    "        confusion_matrix_dt, index=['High', 'Low'],\n",
    "        columns=['High', 'Low'])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true = ytest.to_numpy(),\n",
    "                                 y_score = dt_probs_test[:,1],\n",
    "                                 pos_label=1)\n",
    "auc_dt = np.round(roc_auc_score(y_true = ytest.to_numpy(), y_score = dt_probs_test[:,1]), decimals = 3)\n",
    "\n",
    "    # Random forest\n",
    "confusion_matrix_rf = confusion_matrix(y_true = ytest.to_numpy(), y_pred = rf_pred_class_test)\n",
    "confusion_matrix_rf = confusion_matrix_rf.astype('float') / confusion_matrix_rf.sum(axis=1)[:, np.newaxis]\n",
    "df_cm_rf = pd.DataFrame(\n",
    "        confusion_matrix_rf, index=['High', 'Low'],\n",
    "        columns=['High', 'Low'],)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_true = ytest.to_numpy(),\n",
    "                                 y_score = rf_probs_test[:,1],\n",
    "                                 pos_label=1)\n",
    "auc_rf = np.round(roc_auc_score(y_true = ytest.to_numpy(), y_score = rf_probs_test[:,1]), decimals = 3)\n",
    "    \n",
    "\n",
    "# Plot ROC Curves\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Plot 1: Decision Tree Confusion Matrix\n",
    "sns.heatmap(df_cm_dt, annot=True, fmt='.2f', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Decision Tree Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted Label')\n",
    "axes[0, 0].set_ylabel('True Label')\n",
    "\n",
    "# Plot 2: Random Forest Confusion Matrix\n",
    "sns.heatmap(df_cm_rf, annot=True, fmt='.2f', cmap='Blues', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Random Forest Confusion Matrix')\n",
    "axes[0, 1].set_xlabel('Predicted Label')\n",
    "axes[0, 1].set_ylabel('True Label')\n",
    "\n",
    "# Plot 3: Decision Tree ROC Curve\n",
    "axes[1, 0].plot(fpr, tpr, label=f'AUC = {auc_dt}', color='blue')\n",
    "axes[1, 0].plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line\n",
    "axes[1, 0].set_title('Decision Tree ROC Curve')\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].legend(loc=4)\n",
    "\n",
    "# Plot 4: Random Forest ROC Curve\n",
    "axes[1, 1].plot(fpr2, tpr2, label=f'AUC = {auc_rf}', color='green')\n",
    "axes[1, 1].plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line\n",
    "axes[1, 1].set_title('Random Forest ROC Curve')\n",
    "axes[1, 1].set_xlabel('False Positive Rate')\n",
    "axes[1, 1].set_ylabel('True Positive Rate')\n",
    "axes[1, 1].legend(loc=4)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3: Hyperparameter Tuning**\n",
    "Hyperparameter tuning helps improve model performance by finding the best parameters.\n",
    "\n",
    "1. **Tune XGBoost using GridSearchCV with five folds**:\n",
    "   - Search for the best `n_estimators`, `max_depth`, and `learning_rate:`.\n",
    "\n",
    "2. **Show the best number for each hyperparameter**:\n",
    "\n",
    "3. **Calculate a ROC curve and its corresponding AUC. Compare these results with the previous models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for XGBoost\n",
    "param_grid_xgb = {\"classifier__n_estimators\": np.arange(90,150), \"classifier__max_depth\": np.arange(1,15), \"classifier__learning_rate\": np.arange(0,0.2,0.04)}\n",
    "positive_weight = ytrain.to_pandas().value_counts().iloc[0] / ytrain.to_pandas().value_counts().iloc[1]\n",
    "print(f\"The balanced weight for the loss function is {positive_weight:.3f}\")\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "XGB_Placement = XGBClassifier(max_depth=3,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=100,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=0,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=-1,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            scale_pos_weight=positive_weight,  # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=42        # Seed\n",
    "                            )\n",
    "\n",
    "xgb = Pipeline(steps=[(\"classifier\",XGB_Placement)])\n",
    "xgb.fit(Xtrain, ytrain)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='roc_auc', n_jobs=-1, refit=True,verbose=0)\n",
    "    # Get the best model\n",
    "grid_search_xgb.fit(Xtrain, ytrain) # XGB Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best hyperparameters\n",
    "print(grid_search_xgb.best_params_)\n",
    "# learning rate = 0.08, max depth = 12, n estimators = 110\n",
    "\n",
    "# Calculate AUC and plot ROC curve.\n",
    "xgb_pred_class_test = grid_search_xgb.predict(Xtest)\n",
    "xgb_probs_test = grid_search_xgb.predict_proba(Xtest)[:,1]\n",
    "\n",
    "fpr3, tpr3, thresholds3 = roc_curve(ytest, xgb_probs_test, pos_label=1)\n",
    "auc_xgb = np.round(roc_auc_score(y_true = ytest.to_numpy(), y_score = xgb_probs_test), decimals = 3)\n",
    "\n",
    "plt.plot(fpr3,tpr3,label=f\"XGB Boost, auc={auc_xgb:.3f}\")\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"XGB Boost ROC Curve\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b265b8b",
   "metadata": {},
   "source": [
    "**Written answer:** Compared to the random forest and decision tree models, the XGB boost has a higher AUC, which can be interpreted as the XGB boost being a better fit to predicting the response than the other two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 4: Model Explainability with SHAP**\n",
    "Machine learning models can be difficult to interpret. SHAP helps us understand which features are most important in predictions.\n",
    "\n",
    "1. **Apply SHAP to the best-tuned XGBoost model**.\n",
    "2. **Generate a SHAP Summary Plot**:\n",
    "   - Displays the most important features and their impact.\n",
    "3. **Interpret the results**:\n",
    "   - Explain the plot for the top three features?\n",
    "   - Does the result make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best-tuned XGBoost model from GridSearch\n",
    "best_xgb_model = XGBClassifier(max_depth=12,                 # Depth of each tree\n",
    "                            learning_rate=0.08,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=110,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=0,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=-1,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            scale_pos_weight=positive_weight,  # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=42        # Seed\n",
    "                            )\n",
    "xgbbest = Pipeline(steps=[(\"classifier\", best_xgb_model)])\n",
    "xgbbest.fit(Xtrain, ytrain)\n",
    "\n",
    "# Apply SHAP to the best XGBoost model\n",
    "explainer = shap.TreeExplainer(xgbbest.named_steps[\"classifier\"], data=shap.sample(Xtest,1000), feature_names=X_columns)\n",
    "shap_values = explainer.shap_values(Xtest, check_additivity=False)\n",
    "\n",
    "# Generate SHAP Summary Plot\n",
    "shap.summary_plot(shap_values,\n",
    "                  Xtest,\n",
    "                  show=True, feature_names=X_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer:** The most important features according to the shap summary are **GrLivArea**: Above ground living area in square feet, **YearBuilt**: Year the house was constructed, and **OverallQual**: The overall material and finish quality of the house. \n",
    "\n",
    "- The low values of GrLivArea are very helpful to predict the low price housing. The high values of GrLivArea is fairly helpful to predict the high price housing. \n",
    "- The high values of YearBuilt are extremely helpful to predict the high price housing, while the low values are not very helpful to predict low price housing.\n",
    "- Overall quality has high values that are mostly helpful to predict high price housing with a few perceived outliers that are extremely good predictors. For low values, it is about hte same as for YearBuilt\n",
    "\n",
    "\n",
    "If we want to predict if the housing price is high or low, these do make practical sense. Above ground living area is valued because of factors like sunlight and easy accessibility. Year built ensures that most houses are built to modern standards, so no leaks or parasites (hopefully). Lastly, overall quality is valued because it is the impression of the house on the potential buyers. People want a comfortable and premium house for their budgets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
