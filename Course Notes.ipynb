{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Common computing paradigms: Rules + Data = Answers (Through Computing)\n",
    "\n",
    "Machine Learning: Data + Answers = Rules (Learning!)\n",
    "\n",
    "Using data is a process that repeats in a cycle:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "    - Refer back to **1.** to ensure cohesiveness\n",
    "    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "    - Refer back to **3.** to ensure cohesiveness\n",
    "5. Evaluation\n",
    "    - Return back to **1.** if needed\n",
    "6. Deployment\n",
    "\n",
    "Steps to cleaning data:\n",
    "\n",
    "1. Create variables\n",
    "2. Eliminate redundant variables\n",
    "3. Treat null values\n",
    "4. Treat outliers as valid or invalid\n",
    "\n",
    "**Why do we want to clean data?**\n",
    "\n",
    "- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)\n",
    "- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them\n",
    "- Missing values **should be deleted** when there are too many missing values\n",
    "- Missing values **can be replaced** when you can estimate them using imputation procedures\n",
    "    - For *continuous variables*, replace with median or mean (median more robust to outliers)\n",
    "    - For *nominal variables*, replace with mode\n",
    "    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage\n",
    "- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value\n",
    "    - Some may be hidden in one-dimensional views of multidimensional data\n",
    "    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy\n",
    "- **Valid outliers** and their treatments\n",
    "    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\\pm$ 3 standard deviations\n",
    "    - Truncation based on inter-quartile range: replace all variable values with $Median \\pm 3s, \\text{ where s }= IQR/(2 \\times 0.6745)$\n",
    "    - Truncation using a sigmoid\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "### Input data\n",
    "\n",
    "There are multiple types of numerical data\n",
    "\n",
    "- Continuous, categorical, integer\n",
    "\n",
    "In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$\n",
    "\n",
    "### Target Variables\n",
    "\n",
    "The core of *supervised learning* is that there is a variable $y$ representing the outcome\n",
    "\n",
    "- Continuous $y$ is a supervised regression problem\n",
    "- Discrete $y$ is a supervised classification problem\n",
    "- Binary $y$ is the easiest version of a supervised classification problem\n",
    "\n",
    "If no set $Y$ outcomes exist, then the problem is *unsupervised*\n",
    "\n",
    "## Defining a model and losses\n",
    "\n",
    "A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)\n",
    "\n",
    "**Mean-square error**: $L(Y,\\hat{Y}) = \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Appropriate for regression analysis\n",
    "\n",
    "**Cross entropy**: When you have no understanding, then entropy (chaos) is at its worst. Each additional piece of information is worth less than the one before.\n",
    "\n",
    "We start with one hot vector $y_i$ where it is 1 if in the correct class and 0 otherwise. There are a total of $k$ classes\n",
    "\n",
    "Eg, If you have 3 classes with probabilities $P_A, P_B, P_C$ with a response vector $y_i = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $f(x_i) = (P_a, P_b, P_c)$,\n",
    "\n",
    "- You can isolate the probabilities by setting $(P_a^{y_a} \\times P_b^{y_b} \\times P_c^{y_c})$\n",
    "- Thus, the loss function is: $L(y_i, \\hat{y}_i) = -\\sum_k y_i^k log\\hat{(y_i^k)}$\n",
    "    - If $P_n$ is 0, then we accept that $0log(0) = 0$\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = -y_ilog(\\hat{y}_i) - (1-y_i)log(1-\\hat{y}_i)$ for a binary version\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = - \\sum_k y_i^k log\\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors\n",
    "\n",
    "### Regression functions\n",
    "\n",
    "The \"best\" model is the one that minimizes the expected error over the full distribution of the set of input and output\n",
    "\n",
    "The problem, mathematically, becomes $min_\\beta E[L(Y,f(x|\\beta))]$\n",
    "\n",
    "**Linear regression example**\n",
    "\n",
    "For the whole sample:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "We can then take the least-squares to find the model for our $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "\"Given a distribution with parameters $\\theta$, how likely are my data?\"\n",
    "\n",
    "We assume that:\n",
    "\n",
    "- Assume points are independent\n",
    "- Can be used with any distribution\n",
    "\n",
    "## Properties of MLE\n",
    "\n",
    "- Functional invariance\n",
    "    - $y=f(\\theta) \\rightarrow \\hat y=f(\\hat \\theta)$\n",
    "- Asymptotic properties\n",
    "    - Estimate is asymptotically unbiased\n",
    "    - Estimate is asymptotically efficient (the best model)\n",
    "    - Estimate is asymptotically normall distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "With logistic regression, we can identify how every variable impacts the model.\n",
    "\n",
    "### Classification and regression\n",
    "\n",
    "When you use least-squares, there is no guarantee that your response is between 0 and 1. Also, your target must be normally distributed\n",
    "\n",
    "Logistic regression eliminates ambiguity by pushing middle cases to 0 or 1.\n",
    "    - Intuitively, the odds of an event grow exponentially\n",
    "\n",
    "MLE for logistic regression is **supervised**, so it should have a target. Cases in the sample are independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "We can compare supervised models using test data. The model with better generalizable characteristics will be better. \n",
    "\n",
    "## Training, validation, and test set\n",
    "\n",
    "1. training set is to calibrate the parameters\n",
    "2. validation set: part of training, but used to make decisions of model construction\n",
    "    - Deciding what variables\n",
    "    - Deciding when to stop training\n",
    "3. test set: **Only** used to calculate final metrics. No decision should be made on the test set\n",
    "\n",
    "## Misuse of the test set\n",
    "\n",
    "- Using it to help build your model, using it to remove variables and calibrate the model\n",
    "    - Leads to overly optimistic models\n",
    "- To do with test set:\n",
    "    1. Split at the very beginning\n",
    "    2. Only decision should be model selection\n",
    "    3. If you need to go back to a previous step, resplit training/validation/test set\n",
    "        - Usually not feasible and expensive, but can have serious consequences if leakage occurs\n",
    "- MUST replace test set null values with train set median/mean/parameter, or else there **WILL BE LEAKAGE**\n",
    "\n",
    "## Conditional vs expected test error\n",
    "\n",
    "The **conditional test error** calculates the expectation over different test sets, given a specific training data set. Specific to a given model over a population. \n",
    "\n",
    "- Typically looked at more because it is a fixed model\n",
    "\n",
    "The **expected test error** is taken over both training and test sets, is an average over different model configurations\n",
    "\n",
    "# Decomposing prediction error\n",
    "\n",
    "1. Irreducible error: variability around the true relationship between $x$ and $y$\n",
    "    - This is the best test error we can expect\n",
    "2. Bias: systematic difference of the best fitted model from the true relationship\n",
    "    - $E[\\hat f(x_i)] - f(x_i)$\n",
    "    - Occurs when the function is not the same class of functions fitted\n",
    "    - **Universal approximation property**: some models can approximate any function with enough data: neural networks and tree ensembles\n",
    "3. Variance: variance around the average fit\n",
    "    - $E[\\hat f(x_i) - E[\\hat f(x_i)]]^2$\n",
    "    - If the model is too complex, variance will skyrocket\n",
    "\n",
    "# Bias-variance trade-off\n",
    "\n",
    "We will see that although a more complex model is better for training data, it will not be generalizable for other data sets\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "This is when you have made training error near 0 and thus test error increases a lot\n",
    "\n",
    "Your model should be the best for the data you have and the problem you are trying to solve\n",
    "\n",
    "## Size of the test set\n",
    "\n",
    "- Big enough to detect a difference in test error of interest\n",
    "- Small enough to leave enough data for model fitting\n",
    "    - Rule of thumb: 30% of full data should be allocated to the test data\n",
    "\n",
    "# Measurement in practice\n",
    "\n",
    "1. Performance: how well does the estimated model perform with new observations?\n",
    "2. Decide on how to split the data up: \n",
    "    - Split sample\n",
    "    - N-fold cross validation\n",
    "    - Single sample\n",
    "    - Bootstrapping\n",
    "3. Decide on the performance measure to use\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "### Split sample method\n",
    "\n",
    "- For large data sets with more than 1000 obs, more than 50 groups is sufficient\n",
    "- Distribution of classes should be the same in training and test sets\n",
    "- We need STRICT separation between test and training data\n",
    "\n",
    "### N-fold cross-validation\n",
    "\n",
    "- Typically for smaller data sets under 1000 observations\n",
    "\n",
    "1. Split data into 'N' folds\n",
    "2. Train on 'N-1' folds and test on the remaining fold\n",
    "3. Repeat, where the test set changes every time, thus repeating 'N' times\n",
    "\n",
    "## Out of sample, out of time, out of universe quantitative validation\n",
    "\n",
    "1. Out of sample: test and training data is mixed on one plane\n",
    "    - When mixed with out of time, the data is taken at a different time, so separate\n",
    "2. Out of universe: training and test sets are separated, parallel across time\n",
    "3. Out of time: test set is taken after training or vice versa\n",
    "\n",
    "## Classic performance measures\n",
    "\n",
    "### confusion matrix\n",
    "\n",
    "- can calculate things like sensitivity, specificity, PPV, NPV\n",
    "\n",
    "### ROC curve\n",
    "\n",
    "- AUC larger is better\n",
    "- For multiple classes\n",
    "    - One vs all approach: calculate overall AUC as the weighted average of individual 'n' AUCs\n",
    "    - One vs one approach: calculate nC2 AUCs, then take the average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty management\n",
    "\n",
    "## Estimation and sampling distribution\n",
    "\n",
    "Parameters characterize the population we want to study, like expectations or values that describe the relationship between input and output\n",
    "\n",
    "- Eg, mean, median, slope\n",
    "\n",
    "Sampling distributions are the distribution of an estimator\n",
    "\n",
    "- How good is the estimate of the population parameter?\n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "For a random variable with a normal distribution, the sample and the population have different variance, thus when we use a confidence interval, we use the sampling standard error. \n",
    "\n",
    "A confidence interval states that for every $\\alpha$%, we expect a certain number out of 100 iterations to contain our true mean, variance, median, etc\n",
    "\n",
    "- Variances however require a chi-squared distribution\n",
    "- Confidence intervals do not work with any ranked measures (AUC, ROC curve)\n",
    "\n",
    "## The bootstrap\n",
    "\n",
    "1. Take a sample of the population\n",
    "2. Resample with replacement an equal sized sample from your original sample. \n",
    "3. Your bootstrap statistics will differ each time\n",
    "    - For a statistic $\\theta$, the distribution of the difference between the original sample statistic and your bootstrapped statistics is asymptotically equal to the real difference between the population statistic and your original sample statistic\n",
    "        - $\\delta_i^\\star = \\theta^\\star - \\theta_i^\\star$\n",
    "    - We can estimate the confidence intervals using the quantiles of $\\delta$\n",
    "\n",
    "The bootstrap is a universal technique to obtain confidence intervals and can be applied to any statistics. The confidence intervals are asymptotically correct and the bootstrap does not make assumptions about the underlying distribution\n",
    "\n",
    "Make sure to save checkpoints and your model outputs so you don't have to reload your training each time.\n",
    "\n",
    "- Can be very unstable when the population is small\n",
    "- Takes time to compute\n",
    "- Applicable when measure is not normally distributed and for complex statistics\n",
    "\n",
    "## Parameter uncertainty\n",
    "\n",
    "Calculating confidence intervals of performance measures:\n",
    "\n",
    "1. Take a trained model\n",
    "2. calculate the bootstrap interval over samples with repetitions of the test set\n",
    "3. Take the confidence interval of that specific model's test distribution, use the center as the original test set's estimate\n",
    "\n",
    "calculating confidence intervals of parameter estimates: using bootstrap to calculate regression parameters\n",
    "\n",
    "1. Take a dataset and calculate the bootstrapped samples from one original sample\n",
    "2. Train the full pipeline over the bootstrapped sample\n",
    "    - splitting train and test sets\n",
    "    - Normalize data\n",
    "    - Train the model\n",
    "    - calculate the performance\n",
    "3. calculate the confidence interval for the parameters using the original training parameter estimate\n",
    "\n",
    "## Bootstrap vs cross-validation\n",
    "\n",
    "- Bootstrap is more precise for most cases\n",
    "- If there are less cases than variables, then cross-validation is more robust\n",
    "- At minimum, run 10 by 10 CV, but better is to run 100 by 100\n",
    "    - CV is cheaper than bootstrap\n",
    "- Cross-validation is to tune hyperparameters and select the best model\n",
    "\n",
    "## Prediction uncertainty\n",
    "\n",
    "How certain can we be with a point estimate? We can calculate a confidence interval based on each bootstrap estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row and columnar data formats\n",
    "\n",
    "A new data type that was specially formulated for data scientists\n",
    "\n",
    "## How data is stored\n",
    "\n",
    "Data is stored linearly by the computer in a string of zeroes and ones. \n",
    "\n",
    "We must orient our data to make it make sense to the computer: \n",
    "\n",
    "- Row data storage\n",
    "- Columnar data storage\n",
    "\n",
    "### Row storage (for operational databases)\n",
    "\n",
    "Used in databases, essentially each row from a table all next to each other in one line. Since it is in one row, it is expensive to search all data and replace or delete\n",
    "\n",
    "Eg, Name|Age|Place|Name|Age|Place|Name|Age|Place|\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- each value in the same row are close in space\n",
    "- deleting a row is easy, same as inserting a new row\n",
    "- searching is easy\n",
    "- accessing data is easier\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Expensive and inefficient for analytics\n",
    "\n",
    "### Columnar storage (Superior data method)\n",
    "\n",
    "Each column from a table is next to each other, sequentially. Is much more efficient for calculating over columns. It also has less data space on your disk\n",
    "\n",
    "Eg, Name|Name|Name|Age|Age|Age|Place|Place|Place\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Single instruction operations are very efficient\n",
    "- Modifying columns is faster\n",
    "- Uncompressed data is more efficient\n",
    "- Compressed data is more efficient\n",
    "- Allows for sinlge instruction, multiple data operations of contiguous column data\n",
    "- Data for each column is stored contiguously in memory\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "## Columnar vs row data formats\n",
    "\n",
    "## Spark / Arrow / DuckDB\n",
    "\n",
    "**Apache Parquet** (in disk) and **Apache Arrow** (in memory) are two softwares that store data as columnar\n",
    "\n",
    "Implementations\n",
    "\n",
    "- DuckDB: traditional databases with columnar storage, optimized for data warehousing and storage\n",
    "- Polars: Library for data manipulation, well-structured API that is expressive and easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and regularization\n",
    "\n",
    "## Approaches to feature selection\n",
    "\n",
    "1. Subset selection: identifying a subset of predictors that we believe is related to the response\n",
    "2. Shrinkage/regularization: fitting a model with all predictors but shrinking the coefficients\n",
    "3. Dimension reduction\n",
    "\n",
    "### Sequential selection: greedy searches\n",
    "\n",
    "1. forward selection: start from 0 predictors, then add one each time until the next one we add doesn't help us\n",
    "    - May miss the best model since it doesn't consider the interactions between the predictors\n",
    "2. backward selection: start with all, take off predictors until we cannot feasibly do so anymore\n",
    "    - Usually ends up with more predictors than forward selection\n",
    "3. Stepwise selection: evaluates whether we should add or subtract predictors at each step\n",
    "\n",
    "Greedy searches do not lead to a globally optimal solution\n",
    "\n",
    "### Regularization/shrinkage\n",
    "\n",
    "These approaches work on penalties when the model uses more variables than necessary\n",
    "\n",
    "We define the loss as:\n",
    "\n",
    "$$L(x,y| f) = L_{Bias}(x,y|f) + \\lambda L_{variance}(x,y|f)$$\n",
    "\n",
    "$lambda$ is a **hyperparameter** that can be set to define any configurable part of a model's learning process\n",
    "\n",
    "- It changes the function itself, the outcome, and capacity of the model\n",
    "- It is a modeler's choice and depends on the problem\n",
    "- The $\\lambda$ must be tuned using cross-validation\n",
    "- As $\\lambda$ increases, the variance decreases and the bias increases\n",
    "\n",
    "#### Ridge regression (L2 norm)\n",
    "\n",
    "Imposes a squared loss: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V \\beta_k^2$\n",
    "\n",
    "- variables must be standardized to ensure the model converges to a solution\n",
    "- Does not select variables, but shrinks them to near 0 instead. \n",
    "- Eliminates correlations between variables\n",
    "\n",
    "#### Lasso regression (L1 norm)\n",
    "\n",
    "Imposes an absolute value penalty: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V |\\beta_k|$\n",
    "\n",
    "- can eliminate variables - variable selection\n",
    "- Needs cleaning\n",
    "- Coefficients go to 0 very fast\n",
    "- does not allow for correlation between variables\n",
    "- variables need to be normalized, and it is much more sensitive method to de-normalized variables\n",
    "- good for industries with lots of variables and not that much data\n",
    "\n",
    "#### Elasticnet\n",
    "\n",
    "Imposes an average between ridge and lasso: $\\lambda L_{variance}(x,y|f) = \\lambda \\bigg( \\alpha\\sum_{k=1}^V |\\beta_k| + \\frac{1-\\alpha}{2} \\sum_{k=1}^V \\beta_k^2\\bigg)$\n",
    "\n",
    "- $\\alpha$ is a balance parameter between the weight given to LASSO vs ridge\n",
    "- Typically want to give more weight to LASSO than ridge, unless data is highly correlated\n",
    "- Make sure to read the documentation of elastic net in python since it is not written as it is above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "\n",
    "Trees split the function into sections so it can fit smaller horizontal lines. If let to do infinite cuts, it overfits. It does this by looking at where the next split would yield the best gain in decreasing bias. \n",
    "\n",
    "Trees can be controlled by setting the number of splits, or a minimum number of observations per split\n",
    "\n",
    "Each node is a decision on one feature/predictor.\n",
    "\n",
    "- Trees have higher variance than other methods: running the same process on the same data will yield different results as pictured below\n",
    "\n",
    "![Using the same data, different tree paths](.\\Pictures\\Treesexample1.png)\n",
    "\n",
    "- Trees are unstable! They vary quite a lot\n",
    "- Trees also lack smoothness\n",
    "\n",
    "## Pros and cons to tree methods\n",
    "\n",
    "### Pros\n",
    "\n",
    "1. Trees are highly flexible\n",
    "2. Trees are non-parametric\n",
    "3. Trees are invariant to scale and can handle categorical predictors naturally\n",
    "4. Trees are highly interpretable to non-technical audiences\n",
    "\n",
    "### Cons\n",
    "\n",
    "1. Trees will overfit to the training data\n",
    "    - Thus it is important to cross-validate \n",
    "2. Trees lack smoothness that regression presents\n",
    "\n",
    "### Types of tree methods to reduce variance\n",
    "\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "3. Creating a random forest\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Essentially bootstrapping multiple datasets. \n",
    "\n",
    "### Steps\n",
    "\n",
    "1. For each tree\n",
    "    1. Bootstrap **B** training datasets\n",
    "    2. For the first split, show the tree the data\n",
    "    3. Decide the split based on the features seen\n",
    "    4. Recursively split the nodes with another feature\n",
    "    5. Stop at some stopping criterion. \n",
    "2. For elements $X_{new} = (x_1, x_2, x_3, ..., x_n)$, and trees $t_j$\n",
    "    1. Each tree has its own elements\n",
    "    2. Calculate each tree's estimate $y_j = t_j(x_{new})$\n",
    "3. The final output is the average of all trees: $y_{new} = \\sum_j^B \\frac{t_j(x_{new})}{B}$\n",
    "\n",
    "The average of the average of the datasets have a variance of $\\sigma^2/B$, where **B** is the number of datasets. (This is under an independence assumption)\n",
    "\n",
    "### For classification\n",
    "\n",
    "Trees will result in a vector of proportions $[p1, p2, p3, ...]$, where each proportion will correspond to the proportion of $B$ trees voting for each class\n",
    " \n",
    "    - They are NOT probabilities\n",
    "    - If the bagged classifier has a smooth decision function, just average the decision function values\n",
    "\n",
    "Bagging will **reduce variance** while keeping the same effectiveness for a good predictor, but can make a bad predictor worse\n",
    "\n",
    "### Issue with bagging \n",
    "\n",
    "All **B** trees are correlated, thus benefits of averaging are limited and the variance of the average is actually $\\rho\\sigma^2 + (1-\\rho)\\sigma^2/B$\n",
    "\n",
    "## Random forest\n",
    "\n",
    "Since bagged trees look so similar because they are correlated, we can improve upon this design by taking a random subset of features for each split instead, then basing the split off of one of these random features. Each split will have a different random subset of features\n",
    "\n",
    "Each tree is not the best, but together, they can eliminate the correlation between the nodes of the main tree\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Bootstrap **B** data sets\n",
    "2. Grow **B** trees, at each split, decide based on a random subset\n",
    "3. Average predictions from the **B** trees\n",
    "\n",
    "### How is it better?\n",
    "\n",
    "- Pairs of tree predictions work to reduce correlation because they do not use the same splitting variables. Each tree has its own unique splitting\n",
    "- Easily parallelized\n",
    "- Low bias, low variance than tree or bagging\n",
    "- Can examine how important a feature was to predicting the data\n",
    "- Expensive\n",
    "- Resistant to overfitting\n",
    "    - Will overfit when the data is small\n",
    "- No hyperparameters\n",
    "- no standardization needed\n",
    "\n",
    "### Out of bag error\n",
    "\n",
    "When an observation doesn't get included in the random forest model since it may not appear in the bootstrapped datasets. The model trained on these observations has an \"out of bag error\".\n",
    "\n",
    "Once the out of bag error is stabilized, training is terminated\n",
    "\n",
    "## Stochastic gradient boosting\n",
    "\n",
    "Applies a weak learner (a tree with one split, called a *stump*) in a very neat way in order to learn complex structure. \n",
    "\n",
    "Example to target a number:\n",
    "\n",
    "- Start with one guess, determine if the actual is higher or lower than the estimate\n",
    "- Continue to throw out estimates and continue to determine if the real is higher or lower\n",
    "- \"Trees\" at each point learn from the error of the tree before it\n",
    "    - Each estimate is a separate model that learns from the error of the one before\n",
    "- Each estimate has limited learning itself since it is only one split\n",
    "- Converges to the correct answer\n",
    "\n",
    "Each estimate uses the previous guess and builds on it\n",
    "\n",
    "### Forward stagewise additive modelling\n",
    "\n",
    "Boosting uses basis functions to fit the complex data. In boosting, the basis functions are the weak learners\n",
    "\n",
    "1. Approximate the solution by sequentially adding new basis functions with adjusting parameters and coefficients of previous fit basis functions\n",
    "2. Fit a tree to the residuals from the squared error loss function\n",
    "- Boosting fits an additive model expressed as: $g(x) = \\beta_0 + \\sum_i f_i(x)$\n",
    "    - The basis functions in boosting are the weak learners\n",
    "\n",
    "Ensembles weak learners to make a flexible learner\n",
    "\n",
    "![Forward stagewise additive modelling](.\\Pictures\\forwardstagewise.png)\n",
    "\n",
    "### Boosted Trees\n",
    "\n",
    "- For classification with exponential loss, forward stagewise yields the adaboost.M1 algorithm\n",
    "\n",
    "![Adaboost algorithm](.\\Pictures\\Adaboost.png)\n",
    "\n",
    "- For regression using the squared error loss, the next tree we add is the tree which best fits our residuals\n",
    "- Forward stagewise modelling is a greedy process, so the solutions we get are a greedy approximation to the true minimizer\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "By calculating the gradient of the basis functions, we can learn the local optima. It is a greedy strategy that moves in the direction of steepest descent for reducing loss.\n",
    "\n",
    "Gradient boosting fits a tree to the negative gradient values of the loss using least-squares and a weak learner.\n",
    "\n",
    "![Gradient tree boosting](.\\Pictures\\gradientboost.png)\n",
    "\n",
    "### Random forest vs XGBoost\n",
    "\n",
    "In theory performance should be equal\n",
    "\n",
    "- Random forest requires less tuning\n",
    "- XGBoost can be smaller\n",
    "- XGBoost is more robust to small sample sizes\n",
    "- Random forest is more efficient to train (parallel processing)\n",
    "\n",
    "## Explainability of tree-based ensembles\n",
    "\n",
    "It is difficult to interpret the models shown here:\n",
    "\n",
    "- Black box models do not show the explanation of the patterns on the outputs\n",
    "    - Neural networks, Boosting, Random forest\n",
    "- White box models do show the explanation of the patterns on the outputs\n",
    "    - Decision trees, generalized linear models\n",
    "\n",
    "Non-linear models with complex patterns will normally be black box:\n",
    "\n",
    "Some methods to make them more explainable are:\n",
    "\n",
    "- Variable importance plots\n",
    "- Shapley values\n",
    "- TreeSHAP values\n",
    "\n",
    "### Variable importance plots\n",
    "\n",
    "Shows the statistical impact of the variables in the model as measured by the Gini Index. However, they do not provide an explanation in terms of individual cases.\n",
    "\n",
    "### Shapley values\n",
    "\n",
    "A proportion between the marginal contribution of the variable to a subset of variables, divided by the number of variables in that subset, then summed so that all possible combinations of variables are considered. \n",
    "\n",
    "This is unfortunately an NP-hard calculation\n",
    "\n",
    "### TreeSHAP\n",
    "\n",
    "Using a tree-based approach, we calculate subsets of variables directly, so Shapley values can be calculated over tree cuts\n",
    "\n",
    "- Less computationally expensive\n",
    "- Can not be used for non-tree-based models\n",
    "- Allows for faster calculations than traditional SHAP values\n",
    "\n",
    "Maintains properties of Shapley values:\n",
    "\n",
    "- Local additivity: the shapley value of a subset of values is the sum of the values of each member of the subset\n",
    "- Consistency/monotonicity: the importance of a set of values is larger than the importance of a smaller subset of values that includes all of the original ones.\n",
    "- Missingness: if an attribute importance is zero for all subsets, its shapley value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "- Simplifies data while preserving essential information\n",
    "- Visualizes high-dimensional data in lower dimensions\n",
    "- Improve on performance of algorithms by reducing noise and redundancy\n",
    "\n",
    "## Basis functions\n",
    "\n",
    "1. Can express a function as a linear combination of the basis vectors\n",
    "2. Basis vectors themselves are linearly independent\n",
    "\n",
    "### Principal component analysis\n",
    "\n",
    "This is a method to find the basis vectors that minimize the variance of the observed sample.\n",
    "\n",
    "Properties:\n",
    "\n",
    "1. Finds directions that maximize variance\n",
    "2. Directions are mutually orthogonal (linearly independent)\n",
    "3. The first component will have the highest variance\n",
    "4. The variation present in the principal components decrease as we move from the first to the last\n",
    "5. The principal components are linear combinations of the original variables/basis functions\n",
    "\n",
    "Why do we use principal component analysis/basis functions?\n",
    "\n",
    "- Data compression\n",
    "- Noise reduction\n",
    "- Feature detection for subsequent analysis\n",
    "- Visualization of multidimensional vectors\n",
    "- PCs are uncorrelated features, so correlated features can be interpreted accurate to better characterize the dataset.\n",
    "\n",
    "Formulation of PCAs:\n",
    "\n",
    "$$||x_i||^2 = ||x_i - z_iv||^2 + ||z_iv||^2$$\n",
    "\n",
    "- Length 1: $x_i - x_i^Tv$\n",
    "- Length 2: $z_i = x_i^Tv$\n",
    "- Expressed as distance of a line in space.\n",
    "- Subject to constraint that $v^Tv = 1$\n",
    "- we want to maximize variance so that it can be represented by the basis vectors\n",
    "- minimize error\n",
    "\n",
    "## Singular value decomposition\n",
    "\n",
    "![Singular value decomposition](.\\Pictures\\svd.png)\n",
    "\n",
    "- Used to isolate the forces and noise that act on a function, broke down and able to interpret each part\n",
    "- It always exists\n",
    "- U is an $n \\times k$ matrix with ortho-normal columns $UU^T = I$\n",
    "- V is an ortho-normal $k \\times k$ matrix $V^T = V^{-1}$\n",
    "- S is a $k \\times k$ diagonal matrix, with the non-negative singular values in the diagonal\n",
    "\n",
    "## Sparse PCA\n",
    "\n",
    "$$X\\simeq Z (and) V^T$$\n",
    "\n",
    "- Imposes a sparseness penalty $\\alpha ||V||_1$ or $\\alpha ||V||_1$ on either $Z$ or $V$, which is equal to lasso\n",
    "    - On Z means making each observation loading on a few principal vectors\n",
    "    - On V means making each latent variable only involve a few of the output variables\n",
    "- Principal axes are aligned with main axes\n",
    "- Latent variables are shrunken towards main axes\n",
    "- Aims to find PCs that are sparse, meaning they have many zero loadings\n",
    "- Equal or worse data compression\n",
    "- Sparsity can increase interpretability\n",
    "- Still linear\n",
    "\n",
    "## Latent semantic analysis (LSA)\n",
    "\n",
    "### Topic analysis\n",
    "\n",
    "Text analysis is super difficult. TF-IDF is the frequency of a word in a document, divided by frequency of documents having this term. \"the\" has high frequency, but low TF-IDF. A word like fortuitous would have a higher TF-IDF.\n",
    "\n",
    "Topic analysis is when you use a vector to analyze similarity between topics.\n",
    "\n",
    "### Non-negative matrix factorisation\n",
    "\n",
    "A method that lets us reduce dimensions by imposing constraints that weights and data can only be positive\n",
    "\n",
    "- Automatically clusters the data\n",
    "- incorporates non-negativity constraints to PCA\n",
    "- Solutions are not unique\n",
    "    - We can impose a lasso regularization to make the problem better behaved\n",
    "- Also called **latent semantic analysis**\n",
    "\n",
    "## Centering in dimensionality reduction\n",
    "\n",
    "Centering removes the mean from each data column before dimensionality reduction since the mean drives the direction of the biggest sum of squares, reflected in the first PC \n",
    "\n",
    "When after centering, the first PC reflects the direction with the most variance, which is usually the proper data. \n",
    "\n",
    "For 2 sparse components, centering actually makes a sparse description of the data impossible\n",
    "\n",
    "## Manifold methods\n",
    "\n",
    "A manifold is a locally euclidean space, which means that at small scales it looks flat. Thus there is a bit of distortion when fitting things since they are non-linear\n",
    "\n",
    "### t-SNE: t-distributed stochastic neighbour embedding\n",
    "\n",
    "Defines a probability distribution around each point, which is a manifold method. Used for visualization of high-dimensional data in 2 or 3 dimensions.\n",
    "\n",
    "Aims to preserve pairwise similarities between data points in the low-dimensional embedding. Preserve distance between points\n",
    "\n",
    "- Perplexity: how well a probability distribution predicts a sample, influencing local neighbourhoo dsize considered by t-SNE\n",
    "- probability of j given i: $p_{j|i}$\n",
    "    - Depends on distance between points\n",
    "    - Depends on shape of distribution chosen\n",
    "    - Not symmetric\n",
    "\n",
    "Loss: the kull-back leibler divergence\n",
    "\n",
    "$$D_{p||q} = \\sum_{i\\neq j}p_{ij}log(p_{ij}/q_{ij})$$\n",
    "\n",
    "Shortcomings in plotting high dimensional spaces because of loss of information\n",
    "\n",
    "- Does not scale well\n",
    "- Cannot work on sparse high-dimensional data directly\n",
    "- Very expensive in memory as it works with large dense matrices\n",
    "- Only preserves local structure, must be very careful with perplexity parameter since it depends on it\n",
    "\n",
    "### UMAP\n",
    "\n",
    "Handles the problems from t-SNE, but overall principle is the same. Uses K-nn instead of perplexity\n",
    "\n",
    "Initializes mapping using spectral clustering. \n",
    "\n",
    "- Only locally euclidean (not globally), non-linear\n",
    "- Designed to be more efficientthan t-SNE\n",
    "- Uses binary cross-entropy loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering (unsupervised learning)\n",
    "\n",
    "Assumes well-formed groups and must be normalized\n",
    "\n",
    "- Principal components are linear combinations of the original variables\n",
    "- Principal components minimize variance\n",
    "- Principal components are mutually orthogonal\n",
    "\n",
    "## Distance-based learning\n",
    "\n",
    "- Inputs: data, closeness of instances\n",
    "- Outputs: Classifier, regressor, structure of data, results based on closeness or distance\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering is grouping 'similar' objects together\n",
    "\n",
    "- To establish prototypes or detect outliers\n",
    "- To simplify data for further analysis\n",
    "- To visualize data\n",
    "\n",
    "Clusters are usually never the same, so different clustering can reveal different things about the data\n",
    "\n",
    "- Clustering algorithms:\n",
    "    - Employ some notion of similarity between objects\n",
    "    - Have an explicit or implicit definitions of what a good cluster is\n",
    "    - Optimize that definition to determine the clustering\n",
    "\n",
    "### K-NN\n",
    "\n",
    "In supervised learning, KNN takes the surrounding responses and categorizes the one in question to the most common one of those responses surrounding the data you want to classify. \n",
    "\n",
    "Sometimes, the definition of what is closest can be changed\n",
    "\n",
    "### K-means clustering\n",
    "\n",
    "- Assumes objects are clustered in p-dimensional vectors\n",
    "- Uses distance measure between points\n",
    "\n",
    "Loss function:\n",
    "\n",
    "- To partition the data into $K$ separate subsets,\n",
    "    - Minimizes the euclidean distance loss $$\\sum_{i=1}^K\\sum_{j\\in C_i} ||x_j-c_i||^2$$\n",
    "        - Capital C: $C_i$ represents the disjoint sets that contain the indices of all the features belonging to the ith cluster such that $U_iC_i = (1,\\dots,n)$\n",
    "        - Lower-case c: $c_i$ represents the centroids of the clusters\n",
    "        - If we hold one of the two above fixed, what are the best other variables?\n",
    "    - However, centroid changes for each $i$, so we need to randomly assign centroids that are fixed to assign clusters\n",
    "    - Creates **degenerate problem**: two optimal solutions to a problem, usually avoid by setting a cap\n",
    "- May not converge to an answer, will not always have the same answer\n",
    "    - has many locally optimal solutions, but not a global solution\n",
    "    - Uses a non-convex loss function\n",
    "- Is NP-hard since it tries to find both optimal centroid and data point assignments at the same time\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Inputs: real vectors $x_1, \\dots, x_n$ and K, the desired number of clusters\n",
    "2. Output: a mapping of vectors into $K$ clusters\n",
    "3. Initialize the clusters randomly,\n",
    "4. Repeat the steps below until the clusters stop changing\n",
    "    - Compute centroid of the cluster (mean of all instances within cluster)\n",
    "    - Reassign each instance to the cluster with closest centroid\n",
    "\n",
    "Fast way to partition data into K clusters\n",
    "\n",
    "- Minimizes sum of squared euclidean distances to cluster centroids\n",
    "- Natural way to add new points to existing clusters\n",
    "- Bias towards round, equal size clusters\n",
    "\n",
    "#### Choosing the number of clusters\n",
    "\n",
    "This is a problem that does not converge unless you have perfect parameters\n",
    "\n",
    "1. Elbow method\n",
    "    - Arbitrary selection where adding clusters does not help much \n",
    "    - Sum of variance of clusters is always smaller than variance of entire dataset\n",
    "    - Elbow exists when the curve has maximum curvature, usually the rounded number\n",
    "    - Plot of within-cluster sum of square vs the number of clusters\n",
    "2. Silhouette score\n",
    "    - A measure of how close a point is to other points in its own cluster, compared with the nearest other cluster\n",
    "    - Range of $[-1,1]$, with 1 being closer to its own cluster, while -1 is closer to another cluster\n",
    "    - Clusters with 1 point have a silhouette of 0 by default\n",
    "    - We typically want the clusters to have generally equal lengths and widths\n",
    "\n",
    "## Hierarchical clustering\n",
    "\n",
    "Organizes data objects into a tree-like structure based on similarity\n",
    "\n",
    "- Agglomerative tree construction (bottom up) is most popular\n",
    "- Distance can be chosen to be many things\n",
    "- No natural way of finding which cluster a new point should belong to\n",
    "\n",
    "### Agglomerative clustering\n",
    "\n",
    "Sequential method that does not work with large datasets, depends on linkages\n",
    "\n",
    "- Inputs pairwise distances between a set of data objects\n",
    "- Outputs an assignment of each instance as its own cluster\n",
    "    - Find two clusters in the list that are more similar, remove both from the list, then add their union\n",
    "    - Done until the list only has one single cluster\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Input: pairwise distances between a set of data objects\n",
    "2. Output: a hierarchical clustering\n",
    "3. Assign each instance as its own cluster on a working list $W$\n",
    "4. Repeat until $W$ contains a single cluster with all data objects\n",
    "    - Find two clusters in $W$ that are the most similar\n",
    "    - Add their union to $W$\n",
    "    - Remove the main effects from $W$\n",
    "5. Return all clusters appearing in $W$ at any stage of the algorithm\n",
    "6. Stops after $n-1$ iterations\n",
    "\n",
    "### Cluster dissimilarity\n",
    "\n",
    "- Single linkage (distance between nearest objects)\n",
    "    - prefers spatially extended, longer clusters\n",
    "    - minimum distance between two points in two clusters\n",
    "- Complete linkage (distance between farthest objects)\n",
    "    - prefers compact clusters\n",
    "    - maximum distance between any two points across two clusters\n",
    "- Average linkage (average distance between objects)\n",
    "    - Favours clusters not too big, not too small\n",
    "\n",
    "## Spectral clustering\n",
    "\n",
    "For non-convex data\n",
    "\n",
    "- Searches clusters by pairing points\n",
    "- Clusters are well-defined, even if data is ill-shaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering reading\n",
    "\n",
    "Clustering is done to give a 'first impression' of some set of data. Spectral clustering often outperforms traditional approaches, is simple to implement, and can be solved efficiently using standard linear algebra methods.\n",
    "\n",
    "## Mathematical objects used by spectral clustering\n",
    "\n",
    "### Similarity graphs\n",
    "\n",
    "The basic premise of clustering is to divide the data into groups that are similar within the groups.\n",
    "\n",
    "For each point ($vertex_i$), we can connect it to another point ($vertex_j$) using a similarity variable $s_{ij}$ if the similarity is positive or larger than a certain threshold. The edge is weighted by the similarity variable.\n",
    "\n",
    "We want to find a partition where the weights between groups are low, while the weights within a group are high.\n",
    "\n",
    "In graph notation, this means that the degree of a point is the sum of the weights associated to that point, and the degree matrix is a diagonal matrix with the degrees of each point on the diagonal.\n",
    "\n",
    "Among the different similarity graphs, a few emerge as particularly popular:\n",
    "\n",
    "- $\\epsilon$-neighbourhood graph\n",
    "    - Connects all points whose pairwise distances are smaller than the error term\n",
    "    - Does not present more information about the data to the graph when weighting edges, so it is usually presented as an unweighted graph.\n",
    "- KNN graph\n",
    "    - Neighbourhood relationship is not symmetric, so we resolve through:\n",
    "        - Ignore the directions of the edges\n",
    "        - Only connect two points if they are both within each other's k-nearest neighbours\n",
    "    - esdges are weighted by the similarity of their endpoints like before\n",
    "- Fully connected graph\n",
    "    - Connect all points with positive similarity with each other, weight all edges by the similarity value\n",
    "    - Only useful if similarity function models local neighbourhoods\n",
    "\n",
    "\n",
    "### Graph Laplacians\n",
    "\n",
    "The unnormalized graph Laplacian matrix is defined as: $L=D-W$\n",
    "\n",
    "- Properties of matrix L\n",
    "    - for every vector $f \\in R^n$, we have  $$f'Lf = \\frac{1}{2}\\sum_{i,j=1}^n w_{ij}(f_i - f_j)^2$$\n",
    "    - L is symmetric and positive semi-definite\n",
    "    - The smallest eigenvalue of L is 0, the corresponding eigenvector is the constant 1\n",
    "    - L has n non-negative, real-valued eigenvalues\n",
    "- If G is an undirected graph with non-negative weights, then the multiplicity of the eigenvalues 0 is spanned by the indicator vectors of those components.\n",
    "- The normalized laplacians satisfy the following:\n",
    "    - $L_{sym} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} W D^{-1/2}$\n",
    "    - $L_{rw} = D^{-1}L = I-D^{-1}W$\n",
    "    - for every $f \\in \\mathbb{R}$, $$f'L_{sym}f = \\frac{1}{2}\\sum_{i,j=1}^n \\big(\\frac{f_i}{\\sqrt{d_i}} - \\frac{f_j}{\\sqrt{d_j}}\\big)^2$$\n",
    "    - Lambda is an eigenvalue of $L_{rw}$ with an eigenvector $u$ iff, lambda is also an eigenvalue of $L_{sym}$ with eigenvector $w = D^{-1/2}u$\n",
    "    - Lambda is an eigenvalue of $L_{rw}$ with an eigenvector $u$ iff, lambda AND $u$ can solve the generalized eigenproblem $Lu=\\lambda Du$.\n",
    "    - 0 is an eigenvalue of $L_{rw}$ with an eigenvector of 1. 0 is an eigenvalue of $L_{sym}$ with an eigenvector of $D^{1/2}$\n",
    "    - Both definited of the normalized graph laplacians are positive semi-definite and have $n$ non-negative real-valued eigenvalues\n",
    "\n",
    "\n",
    "## Spectral clustering algorithms\n",
    "\n",
    "- Unnormalized spectral clustering\n",
    "- Normalized spectral clustering (two versions)\n",
    "\n",
    "The algorithms all change the representation of abstract data points into positive points on a graph\n",
    "\n",
    "Considerations:\n",
    "\n",
    "- Should ensure very similar points are closely related in the application context\n",
    "- kNN graph can connect data points across different scales of density\n",
    "\n",
    "## Why do these algorithms work?\n",
    "\n",
    "### Graph partitioning approach\n",
    "\n",
    "- We want to find a partition of the graph such that the edges between different groups have a low weight, while intragroup have high weights as before\n",
    "- Optimize the below functions by finding the minimum\n",
    "\n",
    "- $|A_i|$ is the number of vertices (points)\n",
    "- $Vol(A_i)$ is the weights of the edges\n",
    "\n",
    "$$cut(A_i) = \\frac{1}{2}\\sum_{i=1}^k W(A_i, \\bar A_{i})$$\n",
    "\n",
    "$$Ratiocut(A_i) = \\frac{1}{2}\\sum_{i=1}^k \\frac{W(A_i, \\bar A_i)}{|A_i|} \\\\ = \\sum_{i=1}^k \\frac{cut(A_i, \\bar A_i)}{|A_i|}$$\n",
    "\n",
    "$$Ncut(A_i) = \\sum_{i=1}^k \\frac{cut(A_i, \\bar A_i)}{vol(A_i)}$$\n",
    "\n",
    "![Ratio cut example](.\\Pictures\\ratiocut.png)\n",
    "\n",
    "### Random walk perspective\n",
    "\n",
    "We can show that spectral clustering can be interpreted as containing a random walk within one cluster, that doesn't jump to other clusters often\n",
    "\n",
    "### Perturbation theory approach\n",
    "\n",
    "Perturbation theory looks at how eigenvalues and eigenvectors of a matrix A change if we add a small perturbation $H$. Typically, a certain distance between the perturbed matrix and the original matrix is bounded by a constant times a norm of $H$. \n",
    "\n",
    "## Practical issues surrounding spectral clustering\n",
    "\n",
    "- Eigengap heuristic can be used to help determine the number of clusters (k)\n",
    "- The power method and Krylov methods are commonly used to computer eigenvectors of sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to neural networks and deep learning\n",
    "\n",
    "Neural networks are parallel distributed processor\n",
    "\n",
    "- Natural propensity for storing knowledge and making it available for use\n",
    "- Resembles the brain:\n",
    "    - Knowledge is acquired by the network through a learning process\n",
    "    - Inter-neuron connection strengths known as synaptic weights are used to store the knowledge\n",
    "    \n",
    "## Basic structure\n",
    "\n",
    "- Input layer\n",
    "- Hidden layer: process original patterns using parameters\n",
    "    - Each hidden layer learns more complex patterns\n",
    "- Output layer: takes outputs from hidden layers\n",
    "- Transfer function: functions that collect outputs from previous layers and delivers them to the next\n",
    "\n",
    "### Transfer/activation functions\n",
    "\n",
    "- Needs the capacity to learn\n",
    "- Shapes the neural network\n",
    "- Requirements:\n",
    "    - Monotonic\n",
    "    - Hopefully differentiable\n",
    "    \n",
    "**Softmax function**\n",
    "\n",
    "- Extension of logistic function to multinomial classes\n",
    "- Common as output function of probability estimations\n",
    "\n",
    "**Hyperbolic tangenet activation**\n",
    "\n",
    "- Common in shallow networks and bounded regression\n",
    "- Deemed less plausible than others but helpful when activations are both positive and negative\n",
    "- complex to optimize\n",
    "\n",
    "**ReLU activation**\n",
    "\n",
    "- very popular as a transfer function\n",
    "    - easy to calculate\n",
    "    - regularization occurs at a model level\n",
    "- Extends to softplus and exponential linear units\n",
    "\n",
    "## Loss functions in neural networks\n",
    "\n",
    "Since neural networks are trained in batches, we only use a tiny sample of the training set to make adjustments. A set of batches are an epoch.\n",
    "\n",
    "This means that training is a generalization of stochastic gradient descent.\n",
    "\n",
    "We can change the learning rate to propagate backwards from an error. The learning rate itself is very low because we train on such little sets of data per batch. \n",
    "\n",
    "## Convolutional neural networks\n",
    "\n",
    "CNNs try to mimic how we perceive patterns and how we identify the world visually\n",
    "\n",
    "- appropriate for image classification\n",
    "- built using convolutional layers\n",
    "\n",
    "We need to help computers understand when a certain key identifier is there so it can identify patterns\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "We define a filter as a segment of the image, it will be a neuron in the CNN. It can be represented as a matrix where the units are representative of colours (Eg, white = 1, black = -1)\n",
    "\n",
    "We see if the filter appears or not in an image by performing a convolution. \n",
    "\n",
    "- A convolution multiplies two segments, adds them, then divides by the number of filters applied\n",
    "    - Also known as a dot product\n",
    "- The larger the number we get, the larger the coincidence, which makes the larger the chance for it to be a match\n",
    "\n",
    "We move the filter across the whole image (feature map) to see if there is a fit\n",
    "\n",
    "### Properties of CNNs\n",
    "\n",
    "- Spatial hierarchy\n",
    "    - Initial layers create basic blocks, subsequent layers create complex blocks from the basic ones\n",
    "- useful for all data in which the above properties hold\n",
    "    - Images, audio, time series\n",
    "- Less useful for classic structured data\n",
    "\n",
    "### Defining a convolutional layer\n",
    "\n",
    "- How many feature maps to construct and size of each map\n",
    "    - Training of CNN will decide which features to focus on\n",
    "- Dependent on yourself\n",
    "    - Popular are 5x5, 3x3, 7x7 depending on the size of pixels\n",
    "    \n",
    "**Padding**: a sequence of 0's added to images to get to our desired size of the feature map, enables us to go into the corners if the filter doesn't fit in the feature map cleanly\n",
    "\n",
    "**Stride**: how much to move the filter, smaller strides create more features (both irrelevant and relevant)\n",
    "\n",
    "**Pooling**: a special layer that reduces noise. Popular poolings are max pooling and average pooling. \n",
    "\n",
    "- Max pooling takes the maximum value within a larger segment, then creates a new segment from the collection of a couple of other segments and their maximums\n",
    "\n",
    "![Max pooling with stride 2](.\\Pictures\\maxpooling.png)\n",
    "\n",
    "### Stacking layers: learning complex patterns\n",
    "\n",
    "By stacking layers, we can learn complex features from the combination of simple features\n",
    "\n",
    "## Tensors and input layers\n",
    "\n",
    "**Input layers**: input layers are a vector that work with tensors to generalize undefined dimensions\n",
    "\n",
    "- Layers are always represented by a set of tensors of some dimensions\n",
    "- We *embed* our input into a numeric vector using a numeric transformation - use as many numbers as necessary to preserve the unstructured information\n",
    "\n",
    "For images, we use the image's channels (colours available) where each colour represents the placement of a colour value in some 3D-tensor as shown below, also included are pixel height, width, and channel:\n",
    "\n",
    "![3D-tensor vector & input layer](.\\Pictures\\3dtensor.png)\n",
    "\n",
    "- We usually downsample to reduce complexity, which again comes from what transformation you take to minimize the number of pixels\n",
    "    - From things like maximum pooling or average pooling\n",
    "\n",
    "For time series or signal data, we use a 3D tensor where the structure consists of a variable, timestep, and a value\n",
    "\n",
    "## Architectures\n",
    "\n",
    "The way you assemble the number of layers, strides, pooling, etc is an **architecture**. It depends on a trade-off between available data, problem complexity, desired output, computational capacity, among other things\n",
    "\n",
    "### Example architectures\n",
    "\n",
    "**ZF Net**: a convolutional neural network for 2D images -> Goal: image recognition with 1000 different classes\n",
    "\n",
    "**VGG**: stacked layers of 3x3 maps with stride 1 that converged to 3 dense layers after final max pooling, output a softmax classification -> Takeaway: hierarchical representation is the best way to classify\n",
    "\n",
    "**Google LeNet**: serializes layers by putting more than one filter and pooling\n",
    "\n",
    "**Residual networks**: Uses skip connections to help with the vanishing gradient problem where we get stuck in gradients with 0 values\n",
    "\n",
    "**ResNet**: coming off of the above, we can visualize ResNet along with VGG below, where the left is VGG, center is ResNet with no skip connection, and right is ResNet with skip connections\n",
    "\n",
    "![VGG, ResNet w/o skip, ResNet full](.\\Pictures\\resnetvsvgg.png)\n",
    "\n",
    "### Augmenting datasets for deeper learning\n",
    "\n",
    "We can rotate images or flip them to use them in a different training set, but we should avoid this since the risk of overfitting is very large\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout controls neural network overfitting by setting some weights to 0\n",
    "\n",
    "It is usually implemented as a layer AFTER a dense layer or convolutional layers, we typically like to dropout 40% to 60% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs\n",
    "\n",
    "## Attention\n",
    "\n",
    "Attention is when your brain filters other stimuli to focus your thoughts and vision on what matters at that moment. \n",
    "\n",
    "Attention mechanisms such as LSTMs and GRUs work by weighting inputs so it can make an output more relevant. *Attention is an interface that helps an encoder better measure the information a decoder will use to solve our problem*\n",
    "\n",
    "In general, we can think of it as providing context for language translation since words and position are not enough\n",
    "\n",
    "### Attention process\n",
    "\n",
    "1. Calculate inputs to attention mechanism by processing input data\n",
    "2. Calculate a score that weights the hidden states\n",
    "3. Use a softmax function to add a weight to these scores\n",
    "4. Reweight the hidden states by these weights\n",
    "5. Sum the resulting vectors into a unique context vector\n",
    "6. Pass this as input to the decoder\n",
    "\n",
    "## The transformer\n",
    "\n",
    "Although the idea of linking an attention process between encoder and decoder was the same, the breakthrough was that attention was applied in sequence to stacked encoders and stacked decoders.\n",
    "\n",
    "Additionally, the concept of **Multiheaded self-attention (MHSA)** was introduced, where attention is applied several times with different weights to focus on different aspects of the sequences. You don't necessarily need a lot of heads, most of the heavy lifting involving it is done by a small number of them.\n",
    "\n",
    "### The encoders: \n",
    "\n",
    "Encoders, as we discussed, encode the input for the decoder\n",
    "\n",
    "The steps include\n",
    "\n",
    "1. Embedding input layer to a positional encoding layer\n",
    "2. Normalizing inputs before MHSA\n",
    "3. MHSA itself\n",
    "4. A skip connection (ResNet) to add all attention outputs and the original inputs\n",
    "5. Another normalization layer\n",
    "6. A dense neural network that is applied to each element in the output\n",
    "7. A final skip connection to preserve the original input\n",
    "8. A final normalization layer\n",
    "\n",
    "### Self-attention\n",
    "\n",
    "Attention is calculated on the inputs themselves, so the problem becomes quite complex since it is calculating a set of abstract weights times the inputs to get the key, query, and value sets. It is a quadratic calculation that is very complex\n",
    "\n",
    "#### Multiheaded self-attention\n",
    "\n",
    "MHSA works by creating several heads that all have their own weights, their own weighting function, and their own feed-forward dense network to receive the attentive weights\n",
    "\n",
    "These separations are later combined when passing the position-wise feed-forward layer: concat the outputs of each head into a large matrix, then multiply it be another set of weights that create the input to the feed-forward layer of the encoder\n",
    "\n",
    "1. take an input\n",
    "2. Emebed the words or other things, or start from the output of the encoder\n",
    "3. Split into 8 heads, multiply the embeds with weight matrices for query, value, and key\n",
    "4. calculate attention using the resulting Q, K, V matrices\n",
    "5. concat the resulting matrices then multiply with a final weight matrix to produce the output of the layer\n",
    "\n",
    "### The decoder\n",
    "\n",
    "We take a masked output vector from the encoder and input it into the decoder. The masked output vector hides some of the words of the output and is mixed with an attention procedure with the encoder inputs. \n",
    "\n",
    "We stack decoders to generalize better\n",
    "\n",
    "![Process from encoder to decoder](.\\Pictures\\encodertodecoder.png)\n",
    "\n",
    "### Network top\n",
    "\n",
    "Finally, we add a network top to make it predict, regress, translate, etc\n",
    "\n",
    "## LLMs\n",
    "\n",
    "Generation of new text or things in a sequence uses a large stack of either encoders or decoders. LLMs were born out of this and have been shown to be able to learn conversational language with decoder-only transformer models. \n",
    "\n",
    "### Model training through auto-regression\n",
    "\n",
    "when fed more data,the models are given context and a prompt, the model learns to predict text from the original prompt and what is being generated, penalizing itself when failing to correctly reconstruct text\n",
    "\n",
    "However, this made models less aligned with the user, thus we had to introduce human feedback loops\n",
    "\n",
    "### reinforcement learning from human feedback\n",
    "\n",
    "LLMs should be:\n",
    "\n",
    "- Helpful: help the user solve the task at hand \n",
    "- Honest: should not fabricate information and mislead the user\n",
    "- Harmless: should not cause physical, psychological, or social harm\n",
    "\n",
    "These are some steps to align models:\n",
    "\n",
    "1. collect demonstration data and train a supervised policy\n",
    "    - A prompt is sampled from the prompt dataset\n",
    "    - A labeler demonstrates the desired output behaviour\n",
    "    - Data is used to fine-tune models with supervised learning\n",
    "2. Collect comparison data and train a reward model\n",
    "    - A prompt and several model outputs are sampled\n",
    "    - A labeler ranks the outputs from best to worst\n",
    "    - The data is used to train a reward model\n",
    "3. Optimize a policy against the reward model using reinforcement learning\n",
    "    - A new prompt is sampled\n",
    "    - The policy generates an output\n",
    "    - The reward model calculates a reward for the output\n",
    "    - The reward is used to update the policy through reinforcement learning\n",
    "    - A policy optimization algorithm updates the policy to maximize expected reward\n",
    "    - Process repeats itself until convergence or stopping criteria met\n",
    "\n",
    "## Fine-tuning LLMs\n",
    "\n",
    "We fine-tune LLMs through the quantization of the model: reduction of numerical precision of some of the layers without altering original parameters\n",
    "\n",
    "### QLoRA\n",
    "\n",
    "QLoRA is a finetuning approach that reduces memory usage to finetune a 65 billion parameter model on a single 48 GB GPU while preserving a full 16 bit finetuning task performance.\n",
    "\n",
    "- Back-propagates gradients through a frozen 4-bit quantized pretrained language model into **low rank adapters** (LoRA). The best model family outperforms all previous openly released models on the Vicuna benchmark\n",
    "\n",
    "### Popular quantization methods\n",
    "\n",
    "- **GPTQ**: GPU-optimal, quantizes in a greedy format by compressing the layer with the least amount of quantization first (minimizes the sum of squared errors), then goes through lazy batch updates of the quantized and unquantized weights, then finally is decomposed so that the Hessian is still semi-definitive negative\n",
    "- **GGUF**: CPU-optimal, is an implementation that is optimized for inference in CPU, can use QLoRA\n",
    "- AWQ: combination of the above two\n",
    "\n",
    "### Quantizing in practice\n",
    "\n",
    "1. Create a dataset for fine-tuning, dataset will have both prompt and answer\n",
    "2. Quantize using the dataset as input, train until convergence of the quantize model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
