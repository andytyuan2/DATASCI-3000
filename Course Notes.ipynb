{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Common computing paradigms: Rules + Data = Answers (Through Computing)\n",
    "\n",
    "Machine Learning: Data + Answers = Rules (Learning!)\n",
    "\n",
    "Using data is a process that repeats in a cycle:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "    - Refer back to **1.** to ensure cohesiveness\n",
    "    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "    - Refer back to **3.** to ensure cohesiveness\n",
    "5. Evaluation\n",
    "    - Return back to **1.** if needed\n",
    "6. Deployment\n",
    "\n",
    "Steps to cleaning data:\n",
    "\n",
    "1. Create variables\n",
    "2. Eliminate redundant variables\n",
    "3. Treat null values\n",
    "4. Treat outliers as valid or invalid\n",
    "\n",
    "**Why do we want to clean data?**\n",
    "\n",
    "- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)\n",
    "- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them\n",
    "- Missing values **should be deleted** when there are too many missing values\n",
    "- Missing values **can be replaced** when you can estimate them using imputation procedures\n",
    "    - For *continuous variables*, replace with median or mean (median more robust to outliers)\n",
    "    - For *nominal variables*, replace with mode\n",
    "    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage\n",
    "- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value\n",
    "    - Some may be hidden in one-dimensional views of multidimensional data\n",
    "    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy\n",
    "- **Valid outliers** and their treatments\n",
    "    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\\pm$ 3 standard deviations\n",
    "    - Truncation based on inter-quartile range: replace all variable values with $Median \\pm 3s, \\text{ where s }= IQR/(2 \\times 0.6745)$\n",
    "    - Truncation using a sigmoid\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "### Input data\n",
    "\n",
    "There are multiple types of numerical data\n",
    "\n",
    "- Continuous, categorical, integer\n",
    "\n",
    "In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$\n",
    "\n",
    "### Target Variables\n",
    "\n",
    "The core of *supervised learning* is that there is a variable $y$ representing the outcome\n",
    "\n",
    "- Continuous $y$ is a supervised regression problem\n",
    "- Discrete $y$ is a supervised classification problem\n",
    "- Binary $y$ is the easiest version of a supervised classification problem\n",
    "\n",
    "If no set $Y$ outcomes exist, then the problem is *unsupervised*\n",
    "\n",
    "## Defining a model and losses\n",
    "\n",
    "A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)\n",
    "\n",
    "**Mean-square error**: $L(Y,\\hat{Y}) = \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Appropriate for regression analysis\n",
    "\n",
    "**Cross entropy**: When you have no understanding, then entropy (chaos) is at its worst. Each additional piece of information is worth less than the one before.\n",
    "\n",
    "We start with one hot vector $y_i$ where it is 1 if in the correct class and 0 otherwise. There are a total of $k$ classes\n",
    "\n",
    "Eg, If you have 3 classes with probabilities $P_A, P_B, P_C$ with a response vector $y_i = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $f(x_i) = (P_a, P_b, P_c)$,\n",
    "\n",
    "- You can isolate the probabilities by setting $(P_a^{y_a} \\times P_b^{y_b} \\times P_c^{y_c})$\n",
    "- Thus, the loss function is: $L(y_i, \\hat{y}_i) = -\\sum_k y_i^k log\\hat{(y_i^k)}$\n",
    "    - If $P_n$ is 0, then we accept that $0log(0) = 0$\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = -y_ilog(\\hat{y}_i) - (1-y_i)log(1-\\hat{y}_i)$ for a binary version\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = - \\sum_k y_i^k log\\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors\n",
    "\n",
    "### Regression functions\n",
    "\n",
    "The \"best\" model is the one that minimizes the expected error over the full distribution of the set of input and output\n",
    "\n",
    "The problem, mathematically, becomes $min_\\beta E[L(Y,f(x|\\beta))]$\n",
    "\n",
    "**Linear regression example**\n",
    "\n",
    "For the whole sample:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "We can then take the least-squares to find the model for our $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "\"Given a distribution with parameters $\\theta$, how likely are my data?\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
