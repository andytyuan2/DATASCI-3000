{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Common computing paradigms: Rules + Data = Answers (Through Computing)\n",
    "\n",
    "Machine Learning: Data + Answers = Rules (Learning!)\n",
    "\n",
    "Using data is a process that repeats in a cycle:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "    - Refer back to **1.** to ensure cohesiveness\n",
    "    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "    - Refer back to **3.** to ensure cohesiveness\n",
    "5. Evaluation\n",
    "    - Return back to **1.** if needed\n",
    "6. Deployment\n",
    "\n",
    "Steps to cleaning data:\n",
    "\n",
    "1. Create variables\n",
    "2. Eliminate redundant variables\n",
    "3. Treat null values\n",
    "4. Treat outliers as valid or invalid\n",
    "\n",
    "**Why do we want to clean data?**\n",
    "\n",
    "- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)\n",
    "- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them\n",
    "- Missing values **should be deleted** when there are too many missing values\n",
    "- Missing values **can be replaced** when you can estimate them using imputation procedures\n",
    "    - For *continuous variables*, replace with median or mean (median more robust to outliers)\n",
    "    - For *nominal variables*, replace with mode\n",
    "    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage\n",
    "- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value\n",
    "    - Some may be hidden in one-dimensional views of multidimensional data\n",
    "    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy\n",
    "- **Valid outliers** and their treatments\n",
    "    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\\pm$ 3 standard deviations\n",
    "    - Truncation based on inter-quartile range: replace all variable values with $Median \\pm 3s, \\text{ where s }= IQR/(2 \\times 0.6745)$\n",
    "    - Truncation using a sigmoid\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "### Input data\n",
    "\n",
    "There are multiple types of numerical data\n",
    "\n",
    "- Continuous, categorical, integer\n",
    "\n",
    "In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$\n",
    "\n",
    "### Target Variables\n",
    "\n",
    "The core of *supervised learning* is that there is a variable $y$ representing the outcome\n",
    "\n",
    "- Continuous $y$ is a supervised regression problem\n",
    "- Discrete $y$ is a supervised classification problem\n",
    "- Binary $y$ is the easiest version of a supervised classification problem\n",
    "\n",
    "If no set $Y$ outcomes exist, then the problem is *unsupervised*\n",
    "\n",
    "## Defining a model and losses\n",
    "\n",
    "A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)\n",
    "\n",
    "**Mean-square error**: $L(Y,\\hat{Y}) = \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Appropriate for regression analysis\n",
    "\n",
    "**Cross entropy**: When you have no understanding, then entropy (chaos) is at its worst. Each additional piece of information is worth less than the one before.\n",
    "\n",
    "We start with one hot vector $y_i$ where it is 1 if in the correct class and 0 otherwise. There are a total of $k$ classes\n",
    "\n",
    "Eg, If you have 3 classes with probabilities $P_A, P_B, P_C$ with a response vector $y_i = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $f(x_i) = (P_a, P_b, P_c)$,\n",
    "\n",
    "- You can isolate the probabilities by setting $(P_a^{y_a} \\times P_b^{y_b} \\times P_c^{y_c})$\n",
    "- Thus, the loss function is: $L(y_i, \\hat{y}_i) = -\\sum_k y_i^k log\\hat{(y_i^k)}$\n",
    "    - If $P_n$ is 0, then we accept that $0log(0) = 0$\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = -y_ilog(\\hat{y}_i) - (1-y_i)log(1-\\hat{y}_i)$ for a binary version\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = - \\sum_k y_i^k log\\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors\n",
    "\n",
    "### Regression functions\n",
    "\n",
    "The \"best\" model is the one that minimizes the expected error over the full distribution of the set of input and output\n",
    "\n",
    "The problem, mathematically, becomes $min_\\beta E[L(Y,f(x|\\beta))]$\n",
    "\n",
    "**Linear regression example**\n",
    "\n",
    "For the whole sample:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "We can then take the least-squares to find the model for our $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "\"Given a distribution with parameters $\\theta$, how likely are my data?\"\n",
    "\n",
    "We assume that:\n",
    "\n",
    "- Assume points are independent\n",
    "- Can be used with any distribution\n",
    "\n",
    "## Properties of MLE\n",
    "\n",
    "- Functional invariance\n",
    "    - $y=f(\\theta) \\rightarrow \\hat y=f(\\hat \\theta)$\n",
    "- Asymptotic properties\n",
    "    - Estimate is asymptotically unbiased\n",
    "    - Estimate is asymptotically efficient (the best model)\n",
    "    - Estimate is asymptotically normall distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "With logistic regression, we can identify how every variable impacts the model.\n",
    "\n",
    "### Classification and regression\n",
    "\n",
    "When you use least-squares, there is no guarantee that your response is between 0 and 1. Also, your target must be normally distributed\n",
    "\n",
    "Logistic regression eliminates ambiguity by pushing middle cases to 0 or 1.\n",
    "    - Intuitively, the odds of an event grow exponentially\n",
    "\n",
    "MLE for logistic regression is **supervised**, so it should have a target. Cases in the sample are independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "We can compare supervised models using test data. The model with better generalizable characteristics will be better. \n",
    "\n",
    "## Training, validation, and test set\n",
    "\n",
    "1. training set is to calibrate the parameters\n",
    "2. validation set: part of training, but used to make decisions of model construction\n",
    "    - Deciding what variables\n",
    "    - Deciding when to stop training\n",
    "3. test set: **Only** used to calculate final metrics. No decision should be made on the test set\n",
    "\n",
    "## Misuse of the test set\n",
    "\n",
    "- Using it to help build your model, using it to remove variables and calibrate the model\n",
    "    - Leads to overly optimistic models\n",
    "- To do with test set:\n",
    "    1. Split at the very beginning\n",
    "    2. Only decision should be model selection\n",
    "    3. If you need to go back to a previous step, resplit training/validation/test set\n",
    "        - Usually not feasible and expensive, but can have serious consequences if leakage occurs\n",
    "\n",
    "## Conditional vs expected test error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty management\n",
    "\n",
    "## Estimation and sampling distribution\n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "## The bootstrap\n",
    "\n",
    "Make sure to save checkpoints and your model outputs so you don't have to reload your training each time.\n",
    "\n",
    "## Prediction uncertainty\n",
    "\n",
    "How confident you are about making decisions on your point estimate. Need to save outputs of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row and columnar data formats\n",
    "\n",
    "A new data type that was specially formulated for data scientists\n",
    "\n",
    "## How data is stored\n",
    "\n",
    "Data is stored linearly by the computer in a string of zeroes and ones. \n",
    "\n",
    "We must orient our data to make it make sense to the computer: \n",
    "\n",
    "- Row data storage\n",
    "- Columnar data storage\n",
    "\n",
    "### Row storage (for operational databases)\n",
    "\n",
    "Used in databases, essentially each row from a table all next to each other in one line. Since it is in one row, it is expensive to search all data and replace or delete\n",
    "\n",
    "Eg, Name|Age|Place|Name|Age|Place|Name|Age|Place|\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- each value in the same row are close in space\n",
    "- deleting a row is easy, same as inserting a new row\n",
    "- searching is easy\n",
    "- accessing data is easier\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Expensive and inefficient for analytics\n",
    "\n",
    "### Columnar storage (Superior data method)\n",
    "\n",
    "Each column from a table is next to each other, sequentially. Is much more efficient for calculating over columns. It also has less data space on your disk\n",
    "\n",
    "Eg, Name|Name|Name|Age|Age|Age|Place|Place|Place\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Single instruction operations are very efficient\n",
    "- Modifying columns is faster\n",
    "- Uncompressed data is more effiicent\n",
    "- Compressed data is more efficient\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "## Columnar vs row data formats\n",
    "\n",
    "## Spark / Arrow / DuckDB\n",
    "\n",
    "**Apache Parquet** (in disk) and **Apache Arrow** (in memory) are two softwares that store data as columnar\n",
    "\n",
    "Implementations\n",
    "\n",
    "- DuckDB: traditional databases with columnar storage, optimized for data warehousing and storage\n",
    "- Polars: Library for data manipulation, well-structured API that is expressive and easy to use"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
