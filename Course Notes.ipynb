{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Common computing paradigms: Rules + Data = Answers (Through Computing)\n",
    "\n",
    "Machine Learning: Data + Answers = Rules (Learning!)\n",
    "\n",
    "Using data is a process that repeats in a cycle:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "    - Refer back to **1.** to ensure cohesiveness\n",
    "    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "    - Refer back to **3.** to ensure cohesiveness\n",
    "5. Evaluation\n",
    "    - Return back to **1.** if needed\n",
    "6. Deployment\n",
    "\n",
    "Steps to cleaning data:\n",
    "\n",
    "1. Create variables\n",
    "2. Eliminate redundant variables\n",
    "3. Treat null values\n",
    "4. Treat outliers as valid or invalid\n",
    "\n",
    "**Why do we want to clean data?**\n",
    "\n",
    "- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)\n",
    "- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them\n",
    "- Missing values **should be deleted** when there are too many missing values\n",
    "- Missing values **can be replaced** when you can estimate them using imputation procedures\n",
    "    - For *continuous variables*, replace with median or mean (median more robust to outliers)\n",
    "    - For *nominal variables*, replace with mode\n",
    "    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage\n",
    "- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value\n",
    "    - Some may be hidden in one-dimensional views of multidimensional data\n",
    "    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy\n",
    "- **Valid outliers** and their treatments\n",
    "    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\\pm$ 3 standard deviations\n",
    "    - Truncation based on inter-quartile range: replace all variable values with $Median \\pm 3s, \\text{ where s }= IQR/(2 \\times 0.6745)$\n",
    "    - Truncation using a sigmoid\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "### Input data\n",
    "\n",
    "There are multiple types of numerical data\n",
    "\n",
    "- Continuous, categorical, integer\n",
    "\n",
    "In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$\n",
    "\n",
    "### Target Variables\n",
    "\n",
    "The core of *supervised learning* is that there is a variable $y$ representing the outcome\n",
    "\n",
    "- Continuous $y$ is a supervised regression problem\n",
    "- Discrete $y$ is a supervised classification problem\n",
    "- Binary $y$ is the easiest version of a supervised classification problem\n",
    "\n",
    "If no set $Y$ outcomes exist, then the problem is *unsupervised*\n",
    "\n",
    "## Defining a model and losses\n",
    "\n",
    "A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)\n",
    "\n",
    "**Mean-square error**: $L(Y,\\hat{Y}) = \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Appropriate for regression analysis\n",
    "\n",
    "**Cross entropy**: When you have no understanding, then entropy (chaos) is at its worst. Each additional piece of information is worth less than the one before.\n",
    "\n",
    "We start with one hot vector $y_i$ where it is 1 if in the correct class and 0 otherwise. There are a total of $k$ classes\n",
    "\n",
    "Eg, If you have 3 classes with probabilities $P_A, P_B, P_C$ with a response vector $y_i = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $f(x_i) = (P_a, P_b, P_c)$,\n",
    "\n",
    "- You can isolate the probabilities by setting $(P_a^{y_a} \\times P_b^{y_b} \\times P_c^{y_c})$\n",
    "- Thus, the loss function is: $L(y_i, \\hat{y}_i) = -\\sum_k y_i^k log\\hat{(y_i^k)}$\n",
    "    - If $P_n$ is 0, then we accept that $0log(0) = 0$\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = -y_ilog(\\hat{y}_i) - (1-y_i)log(1-\\hat{y}_i)$ for a binary version\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = - \\sum_k y_i^k log\\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors\n",
    "\n",
    "### Regression functions\n",
    "\n",
    "The \"best\" model is the one that minimizes the expected error over the full distribution of the set of input and output\n",
    "\n",
    "The problem, mathematically, becomes $min_\\beta E[L(Y,f(x|\\beta))]$\n",
    "\n",
    "**Linear regression example**\n",
    "\n",
    "For the whole sample:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "We can then take the least-squares to find the model for our $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "\"Given a distribution with parameters $\\theta$, how likely are my data?\"\n",
    "\n",
    "We assume that:\n",
    "\n",
    "- Assume points are independent\n",
    "- Can be used with any distribution\n",
    "\n",
    "## Properties of MLE\n",
    "\n",
    "- Functional invariance\n",
    "    - $y=f(\\theta) \\rightarrow \\hat y=f(\\hat \\theta)$\n",
    "- Asymptotic properties\n",
    "    - Estimate is asymptotically unbiased\n",
    "    - Estimate is asymptotically efficient (the best model)\n",
    "    - Estimate is asymptotically normall distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "With logistic regression, we can identify how every variable impacts the model.\n",
    "\n",
    "### Classification and regression\n",
    "\n",
    "When you use least-squares, there is no guarantee that your response is between 0 and 1. Also, your target must be normally distributed\n",
    "\n",
    "Logistic regression eliminates ambiguity by pushing middle cases to 0 or 1.\n",
    "    - Intuitively, the odds of an event grow exponentially\n",
    "\n",
    "MLE for logistic regression is **supervised**, so it should have a target. Cases in the sample are independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "We can compare supervised models using test data. The model with better generalizable characteristics will be better. \n",
    "\n",
    "## Training, validation, and test set\n",
    "\n",
    "1. training set is to calibrate the parameters\n",
    "2. validation set: part of training, but used to make decisions of model construction\n",
    "    - Deciding what variables\n",
    "    - Deciding when to stop training\n",
    "3. test set: **Only** used to calculate final metrics. No decision should be made on the test set\n",
    "\n",
    "## Misuse of the test set\n",
    "\n",
    "- Using it to help build your model, using it to remove variables and calibrate the model\n",
    "    - Leads to overly optimistic models\n",
    "- To do with test set:\n",
    "    1. Split at the very beginning\n",
    "    2. Only decision should be model selection\n",
    "    3. If you need to go back to a previous step, resplit training/validation/test set\n",
    "        - Usually not feasible and expensive, but can have serious consequences if leakage occurs\n",
    "- MUST replace test set null values with train set median/mean/parameter, or else there **WILL BE LEAKAGE**\n",
    "\n",
    "## Conditional vs expected test error\n",
    "\n",
    "The conditional test error calculates the expectation over different test sets, given a specific training data set\n",
    "\n",
    "The expected test error is taken over both training and test sets\n",
    "\n",
    "# Decomposing prediction error\n",
    "\n",
    "1. Irreducible error: variability around the true relationship between $x$ and $y$\n",
    "    - This is the best test error we can expect\n",
    "2. Bias: systematic difference of the best fitted model from the true relationship\n",
    "    - $E[\\hat f(x_i)] - f(x_i)$\n",
    "    - Occurs when the function is not the same class of functions fitted\n",
    "    - **Universal approximation property**: some models can approximate any function with enough data: neural networks and tree ensembles\n",
    "3. Variance: variance around the average fit\n",
    "    - $E[\\hat f(x_i) - E[\\hat f(x_i)]]^2$\n",
    "    - If the model is too complex, variance will skyrocket\n",
    "\n",
    "# Bias-variance trade-off\n",
    "\n",
    "We will see that although a more complex model is better for training data, it will not be generalizable for other data sets\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "This is when you have made training error near 0 and thus test error increases a lot\n",
    "\n",
    "Your model should be the best for the data you have and the problem you are trying to solve\n",
    "\n",
    "## Size of the test set\n",
    "\n",
    "- Big enough to detect a difference in test error of interest\n",
    "- Small enough to leave enough data for model fitting\n",
    "    - Rule of thumb: 30% of full data should be allocated to the test data\n",
    "\n",
    "# Measurement in practice\n",
    "\n",
    "1. Performance: how well does the estimated model perform with new observations?\n",
    "2. Decide on how to split the data up: \n",
    "    - Split sample\n",
    "    - N-fold cross validation\n",
    "    - Single sample\n",
    "    - Bootstrapping\n",
    "3. Decide on the performance measure to use\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "### Split sample method\n",
    "\n",
    "- For large data sets with more than 1000 obs, more than 50 groups is sufficient\n",
    "- Distribution of classes should be the same in training and test sets\n",
    "- We need STRICT separation between test and training data\n",
    "\n",
    "### N-fold cross validation\n",
    "\n",
    "- Typically for smaller data sets under 1000 observations\n",
    "\n",
    "1. Split data into 'N' folds\n",
    "2. Train on 'N-1' folds and test on the remaining fold\n",
    "3. Repeat, where the test set changes every time, thus repeating 'N' times\n",
    "\n",
    "## Out of sample, out of time, out of universe quantitative validation\n",
    "\n",
    "1. Out of sample: test and training data is mixed on one plane\n",
    "    - When mixed with out of time, the data is taken at a different time, so separate\n",
    "2. Out of universe: training and test sets are separated, parallel across time\n",
    "3. Out of time: test set is taken after training or vice versa\n",
    "\n",
    "## Classic performance measures\n",
    "\n",
    "### confusion matrix\n",
    "\n",
    "- can calculate things like sensitivity, specificity, PPV, NPV\n",
    "\n",
    "### ROC curve\n",
    "\n",
    "- AUC larger is better\n",
    "- For multiple classes\n",
    "    - One vs all approach: calculate overall AUC as the weighted average of individual 'n' AUCs\n",
    "    - One vs one approach: calculate nC2 AUCs, then take the average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty management\n",
    "\n",
    "## Estimation and sampling distribution\n",
    "\n",
    "Parameters characterize the population we want to study, like expectations or values that describe the relationship between input and output\n",
    "\n",
    "- Eg, mean, median, slope\n",
    "\n",
    "Sampling distributions are the distribution of an estimator\n",
    "\n",
    "- How good is the estimate of the population parameter?a\n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "For a random variable with a normal distribution, the sample and the population have different variance, thus when we use a confidence interval, we use the sampling standard error. \n",
    "\n",
    "A confidence interval states that for every $\\alpha$%, we expect a certain number out of 100 iterations to contain our true mean, variance, median, etc\n",
    "\n",
    "- Variances however require a chi-squared distribution\n",
    "- Confidence intervals do not work with any ranked measures (AUC, ROC curve)\n",
    "\n",
    "## The bootstrap\n",
    "\n",
    "1. Take a sample of the population\n",
    "2. Resample with replacement an equal sized sample from your original sample. \n",
    "3. Your bootstrap statistics will differ each time\n",
    "    - For a statistic $\\theta$, the distribution of the difference between the original sample statistic and your bootstrapped statistics is asymptotically equal to the real difference between the population statistic and your original sample statistic\n",
    "        - $\\delta_i^\\star = \\theta^\\star - \\theta_i^\\star$\n",
    "    - We can estimate the confidence intervals using the quantiles of $\\delta$\n",
    "\n",
    "The bootstrap is a universal technique to obtain confidence intervals and can be applied to any statistics. The confidence intervals are asymptotically correct and the bootstrap does not make assumptions about the underlying distribution\n",
    "\n",
    "Make sure to save checkpoints and your model outputs so you don't have to reload your training each time.\n",
    "\n",
    "- can be very unstable when the population is small\n",
    "- Takes time to compute\n",
    "\n",
    "## Parameter uncertainty\n",
    "\n",
    "Calculating confidence intervals of performance measures:\n",
    "\n",
    "1. Take a trained model\n",
    "2. calculate the bootstrap interval over samples with repetitions of the test set\n",
    "3. Take the confidence interval of that specific model's test distribution, use the center as the original test set's estimate\n",
    "\n",
    "calculating confidence intervals of parameter estimates: using bootstrap to calculate regression parameters\n",
    "\n",
    "1. Take a dataset and calculate the bootstrapped samples from one original sample\n",
    "2. Train the full pipeline over the bootstrapped sample\n",
    "    - splitting train and test sets\n",
    "    - Normalize data\n",
    "    - Train the model\n",
    "    - calculate the performance\n",
    "3. calculate the confidence interval for the parameters using the original training parameter estimate\n",
    "\n",
    "Bootstrap vs cross-validation\n",
    "\n",
    "- Bootstrap is mroe precise for most cases\n",
    "- If there are less cases than variables, then cross-validation is more robust\n",
    "- At minimum, run 10 by 10 CV, but better is to run 100 by 100\n",
    "\n",
    "## Prediction uncertainty\n",
    "\n",
    "How certain can we be with a point estimate? We can calculate a confidence interval based on each bootstrap estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row and columnar data formats\n",
    "\n",
    "A new data type that was specially formulated for data scientists\n",
    "\n",
    "## How data is stored\n",
    "\n",
    "Data is stored linearly by the computer in a string of zeroes and ones. \n",
    "\n",
    "We must orient our data to make it make sense to the computer: \n",
    "\n",
    "- Row data storage\n",
    "- Columnar data storage\n",
    "\n",
    "### Row storage (for operational databases)\n",
    "\n",
    "Used in databases, essentially each row from a table all next to each other in one line. Since it is in one row, it is expensive to search all data and replace or delete\n",
    "\n",
    "Eg, Name|Age|Place|Name|Age|Place|Name|Age|Place|\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- each value in the same row are close in space\n",
    "- deleting a row is easy, same as inserting a new row\n",
    "- searching is easy\n",
    "- accessing data is easier\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Expensive and inefficient for analytics\n",
    "\n",
    "### Columnar storage (Superior data method)\n",
    "\n",
    "Each column from a table is next to each other, sequentially. Is much more efficient for calculating over columns. It also has less data space on your disk\n",
    "\n",
    "Eg, Name|Name|Name|Age|Age|Age|Place|Place|Place\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Single instruction operations are very efficient\n",
    "- Modifying columns is faster\n",
    "- Uncompressed data is more effiicent\n",
    "- Compressed data is more efficient\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "## Columnar vs row data formats\n",
    "\n",
    "## Spark / Arrow / DuckDB\n",
    "\n",
    "**Apache Parquet** (in disk) and **Apache Arrow** (in memory) are two softwares that store data as columnar\n",
    "\n",
    "Implementations\n",
    "\n",
    "- DuckDB: traditional databases with columnar storage, optimized for data warehousing and storage\n",
    "- Polars: Library for data manipulation, well-structured API that is expressive and easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and regularization\n",
    "\n",
    "## Approaches to feature selection\n",
    "\n",
    "1. Subset selection: identifying a subset of predictors that we believe is related to the response\n",
    "2. Shrinkage/regularization: fitting a model with all predictors but shrinking the coefficients\n",
    "3. Dimension reduction\n",
    "\n",
    "### Sequential selection: greedy searches\n",
    "\n",
    "1. forward selection: start from 0 predictors, then add one each time until the next one we add doesn't help us\n",
    "    - May miss the best model since it doesn't consider the interactions between the predictors\n",
    "2. backward selection: start with all, take off predictors until we cannot feasibly do so anymore\n",
    "    - Usually ends up with more predictors than forward selection\n",
    "3. Stepwise selection: evaluates whether we should add or subtract predictors at each step\n",
    "\n",
    "### Regularization/shrinkage\n",
    "\n",
    "These approaches work on penalties when the model uses more variables than necessary\n",
    "\n",
    "We define the loss as:\n",
    "\n",
    "$$L(x,y| f) = L_{Bias}(x,y|f) + \\lambda L_{variance}(x,y|f)$$\n",
    "\n",
    "$lambda$ is a **hyperparameter** that can be set to define any configurable part of a model's learning process\n",
    "\n",
    "- It changes the function itself, the outcome, and capacity of the model\n",
    "- It is a modeler's choice and depends on the problem\n",
    "- The $\\lambda$ must be tuned using cross-validation\n",
    "- As $\\lambda$ increases, the variance decreases and the bias increases\n",
    "\n",
    "#### Ridge regression (L2 norm)\n",
    "\n",
    "Imposes a squared loss: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V \\beta_k^2$\n",
    "\n",
    "- variables must be standardized to ensure the model converges to a solution\n",
    "- Does not select variables, but shrinks them to near 0 instead. \n",
    "- Eliminates correlations between variables\n",
    "\n",
    "#### Lasso regression (L1 norm)\n",
    "\n",
    "Imposes an absolute value penalty: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V |\\beta_k|$\n",
    "\n",
    "- can eliminate variables - variable selection\n",
    "- Needs cleaning\n",
    "- Coefficients go to 0 very fast\n",
    "- does not allow for correlation between variables\n",
    "- variables need to be normalized, and it is much more sensitive method to de-normalized variables\n",
    "- good for industries with lots of variables and not that much data\n",
    "\n",
    "#### Elasticnet\n",
    "\n",
    "Imposes an average between ridge and lasso: $\\lambda L_{variance}(x,y|f) = \\lambda \\bigg( \\alpha\\sum_{k=1}^V |\\beta_k| + \\frac{1-\\alpha}{2} \\sum_{k=1}^V \\beta_k^2\\bigg)$\n",
    "\n",
    "- $\\alpha$ is a balance parameter between the weight given to LASSO vs ridge\n",
    "- Typically want to give more weight to LASSO than ridge, unless data is highly correlated\n",
    "- Make sure to read the documentation of elastic net in python since it is not written as it is above\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
