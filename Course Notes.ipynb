{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Common computing paradigms: Rules + Data = Answers (Through Computing)\n",
    "\n",
    "Machine Learning: Data + Answers = Rules (Learning!)\n",
    "\n",
    "Using data is a process that repeats in a cycle:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "    - Refer back to **1.** to ensure cohesiveness\n",
    "    - Aiming to select variables and clean data (filter outliers, irrelevant variables, delete null values)\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "    - Refer back to **3.** to ensure cohesiveness\n",
    "5. Evaluation\n",
    "    - Return back to **1.** if needed\n",
    "6. Deployment\n",
    "\n",
    "Steps to cleaning data:\n",
    "\n",
    "1. Create variables\n",
    "2. Eliminate redundant variables\n",
    "3. Treat null values\n",
    "4. Treat outliers as valid or invalid\n",
    "\n",
    "**Why do we want to clean data?**\n",
    "\n",
    "- Dirty data that doesn't make sense in our context (Eg, age being negative, is 0 a true value or absence of something, incomplete data, duplicate)\n",
    "- Missing values **can be kept** since it may be valuable. It is good practice to create an additional category or indicator variable for them\n",
    "- Missing values **should be deleted** when there are too many missing values\n",
    "- Missing values **can be replaced** when you can estimate them using imputation procedures\n",
    "    - For *continuous variables*, replace with median or mean (median more robust to outliers)\n",
    "    - For *nominal variables*, replace with mode\n",
    "    - For *regression* or *tree-based imputation*, predict using other variables; cannot use target class as predictor if missing values can occur during model usage\n",
    "- Outliers **can be kept** as null values or **deleted** when they are invalid observations or out of distribution due to low sample value\n",
    "    - Some may be hidden in one-dimensional views of multidimensional data\n",
    "    - Invalid observations are typically treated as null values, while out of distribution are typically removed and a note is made in the policy\n",
    "- **Valid outliers** and their treatments\n",
    "    - Truncation based on z-scores: replace all variable values with z-scores above or below 3 with the mean $\\pm$ 3 standard deviations\n",
    "    - Truncation based on inter-quartile range: replace all variable values with $Median \\pm 3s, \\text{ where s }= IQR/(2 \\times 0.6745)$\n",
    "    - Truncation using a sigmoid\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "### Input data\n",
    "\n",
    "There are multiple types of numerical data\n",
    "\n",
    "- Continuous, categorical, integer\n",
    "\n",
    "In general, we will assume that for each individual sample $i$, there is a vector $x_i$ that stores the information of $p$ variables that represent that individual. We have a sample $X$, the design matrix, of cardinality $I$ such that for every $i$ we know their information $x_i$\n",
    "\n",
    "### Target Variables\n",
    "\n",
    "The core of *supervised learning* is that there is a variable $y$ representing the outcome\n",
    "\n",
    "- Continuous $y$ is a supervised regression problem\n",
    "- Discrete $y$ is a supervised classification problem\n",
    "- Binary $y$ is the easiest version of a supervised classification problem\n",
    "\n",
    "If no set $Y$ outcomes exist, then the problem is *unsupervised*\n",
    "\n",
    "## Defining a model and losses\n",
    "\n",
    "A statistical model takes an input and gives an output. A model minimizes loss or error function by measuring the goodness of fit of the function. There are many examples of loss minimizing functions, such as *mean-square error* or *cross-entropy*(MLE)\n",
    "\n",
    "**Mean-square error**: $L(Y,\\hat{Y}) = \\sum_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- Appropriate for regression analysis\n",
    "\n",
    "**Cross entropy**: When you have no understanding, then entropy (chaos) is at its worst. Each additional piece of information is worth less than the one before.\n",
    "\n",
    "We start with one hot vector $y_i$ where it is 1 if in the correct class and 0 otherwise. There are a total of $k$ classes\n",
    "\n",
    "Eg, If you have 3 classes with probabilities $P_A, P_B, P_C$ with a response vector $y_i = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ and $f(x_i) = (P_a, P_b, P_c)$,\n",
    "\n",
    "- You can isolate the probabilities by setting $(P_a^{y_a} \\times P_b^{y_b} \\times P_c^{y_c})$\n",
    "- Thus, the loss function is: $L(y_i, \\hat{y}_i) = -\\sum_k y_i^k log\\hat{(y_i^k)}$\n",
    "    - If $P_n$ is 0, then we accept that $0log(0) = 0$\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = -y_ilog(\\hat{y}_i) - (1-y_i)log(1-\\hat{y}_i)$ for a binary version\n",
    "\n",
    "$l(y_i, \\hat{y}_i) = - \\sum_k y_i^k log\\hat{(y_i^k)}$ for multinomial cross-entropy, where $k$ represents the number of vectors\n",
    "\n",
    "### Regression functions\n",
    "\n",
    "The \"best\" model is the one that minimizes the expected error over the full distribution of the set of input and output\n",
    "\n",
    "The problem, mathematically, becomes $min_\\beta E[L(Y,f(x|\\beta))]$\n",
    "\n",
    "**Linear regression example**\n",
    "\n",
    "For the whole sample:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "We can then take the least-squares to find the model for our $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "\"Given a distribution with parameters $\\theta$, how likely are my data?\"\n",
    "\n",
    "We assume that:\n",
    "\n",
    "- Assume points are independent\n",
    "- Can be used with any distribution\n",
    "\n",
    "## Properties of MLE\n",
    "\n",
    "- Functional invariance\n",
    "    - $y=f(\\theta) \\rightarrow \\hat y=f(\\hat \\theta)$\n",
    "- Asymptotic properties\n",
    "    - Estimate is asymptotically unbiased\n",
    "    - Estimate is asymptotically efficient (the best model)\n",
    "    - Estimate is asymptotically normall distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "With logistic regression, we can identify how every variable impacts the model.\n",
    "\n",
    "### Classification and regression\n",
    "\n",
    "When you use least-squares, there is no guarantee that your response is between 0 and 1. Also, your target must be normally distributed\n",
    "\n",
    "Logistic regression eliminates ambiguity by pushing middle cases to 0 or 1.\n",
    "    - Intuitively, the odds of an event grow exponentially\n",
    "\n",
    "MLE for logistic regression is **supervised**, so it should have a target. Cases in the sample are independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "We can compare supervised models using test data. The model with better generalizable characteristics will be better. \n",
    "\n",
    "## Training, validation, and test set\n",
    "\n",
    "1. training set is to calibrate the parameters\n",
    "2. validation set: part of training, but used to make decisions of model construction\n",
    "    - Deciding what variables\n",
    "    - Deciding when to stop training\n",
    "3. test set: **Only** used to calculate final metrics. No decision should be made on the test set\n",
    "\n",
    "## Misuse of the test set\n",
    "\n",
    "- Using it to help build your model, using it to remove variables and calibrate the model\n",
    "    - Leads to overly optimistic models\n",
    "- To do with test set:\n",
    "    1. Split at the very beginning\n",
    "    2. Only decision should be model selection\n",
    "    3. If you need to go back to a previous step, resplit training/validation/test set\n",
    "        - Usually not feasible and expensive, but can have serious consequences if leakage occurs\n",
    "- MUST replace test set null values with train set median/mean/parameter, or else there **WILL BE LEAKAGE**\n",
    "\n",
    "## Conditional vs expected test error\n",
    "\n",
    "The **conditional test error** calculates the expectation over different test sets, given a specific training data set. Specific to a given model over a population. \n",
    "\n",
    "- Typically looked at more because it is a fixed model\n",
    "\n",
    "The **expected test error** is taken over both training and test sets, is an average over different model configurations\n",
    "\n",
    "# Decomposing prediction error\n",
    "\n",
    "1. Irreducible error: variability around the true relationship between $x$ and $y$\n",
    "    - This is the best test error we can expect\n",
    "2. Bias: systematic difference of the best fitted model from the true relationship\n",
    "    - $E[\\hat f(x_i)] - f(x_i)$\n",
    "    - Occurs when the function is not the same class of functions fitted\n",
    "    - **Universal approximation property**: some models can approximate any function with enough data: neural networks and tree ensembles\n",
    "3. Variance: variance around the average fit\n",
    "    - $E[\\hat f(x_i) - E[\\hat f(x_i)]]^2$\n",
    "    - If the model is too complex, variance will skyrocket\n",
    "\n",
    "# Bias-variance trade-off\n",
    "\n",
    "We will see that although a more complex model is better for training data, it will not be generalizable for other data sets\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "This is when you have made training error near 0 and thus test error increases a lot\n",
    "\n",
    "Your model should be the best for the data you have and the problem you are trying to solve\n",
    "\n",
    "## Size of the test set\n",
    "\n",
    "- Big enough to detect a difference in test error of interest\n",
    "- Small enough to leave enough data for model fitting\n",
    "    - Rule of thumb: 30% of full data should be allocated to the test data\n",
    "\n",
    "# Measurement in practice\n",
    "\n",
    "1. Performance: how well does the estimated model perform with new observations?\n",
    "2. Decide on how to split the data up: \n",
    "    - Split sample\n",
    "    - N-fold cross validation\n",
    "    - Single sample\n",
    "    - Bootstrapping\n",
    "3. Decide on the performance measure to use\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "### Split sample method\n",
    "\n",
    "- For large data sets with more than 1000 obs, more than 50 groups is sufficient\n",
    "- Distribution of classes should be the same in training and test sets\n",
    "- We need STRICT separation between test and training data\n",
    "\n",
    "### N-fold cross-validation\n",
    "\n",
    "- Typically for smaller data sets under 1000 observations\n",
    "\n",
    "1. Split data into 'N' folds\n",
    "2. Train on 'N-1' folds and test on the remaining fold\n",
    "3. Repeat, where the test set changes every time, thus repeating 'N' times\n",
    "\n",
    "## Out of sample, out of time, out of universe quantitative validation\n",
    "\n",
    "1. Out of sample: test and training data is mixed on one plane\n",
    "    - When mixed with out of time, the data is taken at a different time, so separate\n",
    "2. Out of universe: training and test sets are separated, parallel across time\n",
    "3. Out of time: test set is taken after training or vice versa\n",
    "\n",
    "## Classic performance measures\n",
    "\n",
    "### confusion matrix\n",
    "\n",
    "- can calculate things like sensitivity, specificity, PPV, NPV\n",
    "\n",
    "### ROC curve\n",
    "\n",
    "- AUC larger is better\n",
    "- For multiple classes\n",
    "    - One vs all approach: calculate overall AUC as the weighted average of individual 'n' AUCs\n",
    "    - One vs one approach: calculate nC2 AUCs, then take the average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty management\n",
    "\n",
    "## Estimation and sampling distribution\n",
    "\n",
    "Parameters characterize the population we want to study, like expectations or values that describe the relationship between input and output\n",
    "\n",
    "- Eg, mean, median, slope\n",
    "\n",
    "Sampling distributions are the distribution of an estimator\n",
    "\n",
    "- How good is the estimate of the population parameter?\n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "For a random variable with a normal distribution, the sample and the population have different variance, thus when we use a confidence interval, we use the sampling standard error. \n",
    "\n",
    "A confidence interval states that for every $\\alpha$%, we expect a certain number out of 100 iterations to contain our true mean, variance, median, etc\n",
    "\n",
    "- Variances however require a chi-squared distribution\n",
    "- Confidence intervals do not work with any ranked measures (AUC, ROC curve)\n",
    "\n",
    "## The bootstrap\n",
    "\n",
    "1. Take a sample of the population\n",
    "2. Resample with replacement an equal sized sample from your original sample. \n",
    "3. Your bootstrap statistics will differ each time\n",
    "    - For a statistic $\\theta$, the distribution of the difference between the original sample statistic and your bootstrapped statistics is asymptotically equal to the real difference between the population statistic and your original sample statistic\n",
    "        - $\\delta_i^\\star = \\theta^\\star - \\theta_i^\\star$\n",
    "    - We can estimate the confidence intervals using the quantiles of $\\delta$\n",
    "\n",
    "The bootstrap is a universal technique to obtain confidence intervals and can be applied to any statistics. The confidence intervals are asymptotically correct and the bootstrap does not make assumptions about the underlying distribution\n",
    "\n",
    "Make sure to save checkpoints and your model outputs so you don't have to reload your training each time.\n",
    "\n",
    "- Can be very unstable when the population is small\n",
    "- Takes time to compute\n",
    "- Applicable when measure is not normally distributed and for complex statistics\n",
    "\n",
    "## Parameter uncertainty\n",
    "\n",
    "Calculating confidence intervals of performance measures:\n",
    "\n",
    "1. Take a trained model\n",
    "2. calculate the bootstrap interval over samples with repetitions of the test set\n",
    "3. Take the confidence interval of that specific model's test distribution, use the center as the original test set's estimate\n",
    "\n",
    "calculating confidence intervals of parameter estimates: using bootstrap to calculate regression parameters\n",
    "\n",
    "1. Take a dataset and calculate the bootstrapped samples from one original sample\n",
    "2. Train the full pipeline over the bootstrapped sample\n",
    "    - splitting train and test sets\n",
    "    - Normalize data\n",
    "    - Train the model\n",
    "    - calculate the performance\n",
    "3. calculate the confidence interval for the parameters using the original training parameter estimate\n",
    "\n",
    "## Bootstrap vs cross-validation\n",
    "\n",
    "- Bootstrap is more precise for most cases\n",
    "- If there are less cases than variables, then cross-validation is more robust\n",
    "- At minimum, run 10 by 10 CV, but better is to run 100 by 100\n",
    "    - CV is cheaper than bootstrap\n",
    "- Cross-validation is to tune hyperparameters and select the best model\n",
    "\n",
    "## Prediction uncertainty\n",
    "\n",
    "How certain can we be with a point estimate? We can calculate a confidence interval based on each bootstrap estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row and columnar data formats\n",
    "\n",
    "A new data type that was specially formulated for data scientists\n",
    "\n",
    "## How data is stored\n",
    "\n",
    "Data is stored linearly by the computer in a string of zeroes and ones. \n",
    "\n",
    "We must orient our data to make it make sense to the computer: \n",
    "\n",
    "- Row data storage\n",
    "- Columnar data storage\n",
    "\n",
    "### Row storage (for operational databases)\n",
    "\n",
    "Used in databases, essentially each row from a table all next to each other in one line. Since it is in one row, it is expensive to search all data and replace or delete\n",
    "\n",
    "Eg, Name|Age|Place|Name|Age|Place|Name|Age|Place|\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- each value in the same row are close in space\n",
    "- deleting a row is easy, same as inserting a new row\n",
    "- searching is easy\n",
    "- accessing data is easier\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Expensive and inefficient for analytics\n",
    "\n",
    "### Columnar storage (Superior data method)\n",
    "\n",
    "Each column from a table is next to each other, sequentially. Is much more efficient for calculating over columns. It also has less data space on your disk\n",
    "\n",
    "Eg, Name|Name|Name|Age|Age|Age|Place|Place|Place\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Single instruction operations are very efficient\n",
    "- Modifying columns is faster\n",
    "- Uncompressed data is more efficient\n",
    "- Compressed data is more efficient\n",
    "- Allows for sinlge instruction, multiple data operations of contiguous column data\n",
    "- Data for each column is stored contiguously in memory\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "## Columnar vs row data formats\n",
    "\n",
    "## Spark / Arrow / DuckDB\n",
    "\n",
    "**Apache Parquet** (in disk) and **Apache Arrow** (in memory) are two softwares that store data as columnar\n",
    "\n",
    "Implementations\n",
    "\n",
    "- DuckDB: traditional databases with columnar storage, optimized for data warehousing and storage\n",
    "- Polars: Library for data manipulation, well-structured API that is expressive and easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and regularization\n",
    "\n",
    "## Approaches to feature selection\n",
    "\n",
    "1. Subset selection: identifying a subset of predictors that we believe is related to the response\n",
    "2. Shrinkage/regularization: fitting a model with all predictors but shrinking the coefficients\n",
    "3. Dimension reduction\n",
    "\n",
    "### Sequential selection: greedy searches\n",
    "\n",
    "1. forward selection: start from 0 predictors, then add one each time until the next one we add doesn't help us\n",
    "    - May miss the best model since it doesn't consider the interactions between the predictors\n",
    "2. backward selection: start with all, take off predictors until we cannot feasibly do so anymore\n",
    "    - Usually ends up with more predictors than forward selection\n",
    "3. Stepwise selection: evaluates whether we should add or subtract predictors at each step\n",
    "\n",
    "Greedy searches do not lead to a globally optimal solution\n",
    "\n",
    "### Regularization/shrinkage\n",
    "\n",
    "These approaches work on penalties when the model uses more variables than necessary\n",
    "\n",
    "We define the loss as:\n",
    "\n",
    "$$L(x,y| f) = L_{Bias}(x,y|f) + \\lambda L_{variance}(x,y|f)$$\n",
    "\n",
    "$lambda$ is a **hyperparameter** that can be set to define any configurable part of a model's learning process\n",
    "\n",
    "- It changes the function itself, the outcome, and capacity of the model\n",
    "- It is a modeler's choice and depends on the problem\n",
    "- The $\\lambda$ must be tuned using cross-validation\n",
    "- As $\\lambda$ increases, the variance decreases and the bias increases\n",
    "\n",
    "#### Ridge regression (L2 norm)\n",
    "\n",
    "Imposes a squared loss: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V \\beta_k^2$\n",
    "\n",
    "- variables must be standardized to ensure the model converges to a solution\n",
    "- Does not select variables, but shrinks them to near 0 instead. \n",
    "- Eliminates correlations between variables\n",
    "\n",
    "#### Lasso regression (L1 norm)\n",
    "\n",
    "Imposes an absolute value penalty: $\\lambda L_{variance}(x,y|f) = \\lambda \\sum_{k=1}^V |\\beta_k|$\n",
    "\n",
    "- can eliminate variables - variable selection\n",
    "- Needs cleaning\n",
    "- Coefficients go to 0 very fast\n",
    "- does not allow for correlation between variables\n",
    "- variables need to be normalized, and it is much more sensitive method to de-normalized variables\n",
    "- good for industries with lots of variables and not that much data\n",
    "\n",
    "#### Elasticnet\n",
    "\n",
    "Imposes an average between ridge and lasso: $\\lambda L_{variance}(x,y|f) = \\lambda \\bigg( \\alpha\\sum_{k=1}^V |\\beta_k| + \\frac{1-\\alpha}{2} \\sum_{k=1}^V \\beta_k^2\\bigg)$\n",
    "\n",
    "- $\\alpha$ is a balance parameter between the weight given to LASSO vs ridge\n",
    "- Typically want to give more weight to LASSO than ridge, unless data is highly correlated\n",
    "- Make sure to read the documentation of elastic net in python since it is not written as it is above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "\n",
    "Trees split the function into sections so it can fit smaller horizontal lines. If let to do infinite cuts, it overfits. It does this by looking at where the next split would yield the best gain in decreasing bias. \n",
    "\n",
    "Trees can be controlled by setting the number of splits, or a minimum number of observations per split\n",
    "\n",
    "Each node is a decision on one feature/predictor.\n",
    "\n",
    "- Trees have higher variance than other methods: running the same process on the same data will yield different results as pictured below\n",
    "\n",
    "![Using the same data, different tree paths](.\\Pictures\\Treesexample1.png)\n",
    "\n",
    "- Trees are unstable! They vary quite a lot\n",
    "- Trees also lack smoothness\n",
    "\n",
    "## Pros and cons to tree methods\n",
    "\n",
    "### Pros\n",
    "\n",
    "1. Trees are highly flexible\n",
    "2. Trees are non-parametric\n",
    "3. Trees are invariant to scale and can handle categorical predictors naturally\n",
    "4. Trees are highly interpretable to non-technical audiences\n",
    "\n",
    "### Cons\n",
    "\n",
    "1. Trees will overfit to the training data\n",
    "    - Thus it is important to cross-validate \n",
    "2. Trees lack smoothness that regression presents\n",
    "\n",
    "### Types of tree methods to reduce variance\n",
    "\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "3. Creating a random forest\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Essentially bootstrapping multiple datasets. \n",
    "\n",
    "### Steps\n",
    "\n",
    "1. For each tree\n",
    "    1. Bootstrap **B** training datasets\n",
    "    2. For the first split, show the tree the data\n",
    "    3. Decide the split based on the features seen\n",
    "    4. Recursively split the nodes with another feature\n",
    "    5. Stop at some stopping criterion. \n",
    "2. For elements $X_{new} = (x_1, x_2, x_3, ..., x_n)$, and trees $t_j$\n",
    "    1. Each tree has its own elements\n",
    "    2. Calculate each tree's estimate $y_j = t_j(x_{new})$\n",
    "3. The final output is the average of all trees: $y_{new} = \\sum_j^B \\frac{t_j(x_{new})}{B}$\n",
    "\n",
    "The average of the average of the datasets have a variance of $\\sigma^2/B$, where **B** is the number of datasets. (This is under an independence assumption)\n",
    "\n",
    "### For classification\n",
    "\n",
    "Trees will result in a vector of proportions $[p1, p2, p3, ...]$, where each proportion will correspond to the proportion of $B$ trees voting for each class\n",
    " \n",
    "    - They are NOT probabilities\n",
    "    - If the bagged classifier has a smooth decision function, just average the decision function values\n",
    "\n",
    "Bagging will reduce variance while keeping the same effectiveness for a good predictor, but can make a bad predictor worse\n",
    "\n",
    "### Issue with bagging \n",
    "\n",
    "All **B** trees are correlated, thus benefits of averaging are limited and the variance of the average is actually $\\rho\\sigma^2 + (1-\\rho)\\sigma^2/B$\n",
    "\n",
    "## Random forest\n",
    "\n",
    "Since bagged trees look so similar because they are correlated, we can improve upon this design by taking a random subset of features for each split instead, then basing the split off of one of these random features. Each split will have a different random subset of features\n",
    "\n",
    "Each tree is not the best, but together, they can eliminate the correlation between the nodes of the main tree\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Bootstrap **B** data sets\n",
    "2. Grow **B** trees, at each split, decide based on a random subset\n",
    "3. Average predictions from the **B** trees\n",
    "\n",
    "### How is it better?\n",
    "\n",
    "- Pairs of tree predictions work to reduce correlation because they do not use the same splitting variables. Each tree has its own unique splitting\n",
    "- Easily parallelized\n",
    "- Low bias, low variance than tree or bagging\n",
    "- Can examine how important a feature was to predicting the data\n",
    "- Expensive\n",
    "- Resistant to overfitting\n",
    "    - Will overfit when the data is small\n",
    "- No hyperparameters\n",
    "- no standardization needed\n",
    "\n",
    "### Out of bag error\n",
    "\n",
    "When an observation doesn't get included in the random forest model since it may not appear in the bootstrapped datasets. The model trained on these observations has an \"out of bag error\".\n",
    "\n",
    "Once the out of bag error is stabilized, training is terminated\n",
    "\n",
    "## Stochastic gradient boosting\n",
    "\n",
    "Applies a weak learner (a tree with one split, called a *stump*) in a very neat way in order to learn complex structure. \n",
    "\n",
    "Example to target a number:\n",
    "\n",
    "    - Start with one guess, determine if the actual is higher or lower than the estimate\n",
    "    - Continue to throw out estimates and continue to determine if the real is higher or lower\n",
    "    - \"Trees\" at each point learn from the error of the tree before it\n",
    "        - Each estimate is a separate model that learns from the error of the one before\n",
    "    - Each estimate has limited learning itself since it is only one split\n",
    "    - Converges to the correct answer\n",
    "\n",
    "Each estimate uses the previous guess and builds on it\n",
    "\n",
    "### Forward stagewise additive modelling\n",
    "\n",
    "Boosting uses basis functions to fit the complex data. In boosting, the basis functions are the weak learners\n",
    "\n",
    "1. Approximate the solution by sequentially adding new basis functions with adjusting parameters and coefficients of previous fit basis functions\n",
    "2. Fit a tree to the residuals from the squared error loss function\n",
    "- Boosting fits an additive model expressed as: $g(x) = \\beta_0 + \\sum_i f_i(x)$\n",
    "    - The basis functions in boosting are the weak learners\n",
    "\n",
    "Ensembles weak learners to make a flexible learner\n",
    "\n",
    "![Forward stagewise additive modelling](.\\Pictures\\forwardstagewise.png)\n",
    "\n",
    "### Boosted Trees\n",
    "\n",
    "- For classification with exponential loss, forward stagewise yields the adaboost.M1 algorithm\n",
    "\n",
    "![Adaboost algorithm](.\\Pictures\\Adaboost.png)\n",
    "\n",
    "- For regression using the squared error loss, the next tree we add is the tree which best fits our residuals\n",
    "- Forward stagewise modelling is a greedy process, so the solutions we get are a greedy approximation to the true minimizer\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "By calculating the gradient of the basis functions, we can learn the local optima. It is a greedy strategy that moves in the direction of steepest descent for reducing loss.\n",
    "\n",
    "Gradient boosting fits a tree to the negative gradient values of the loss using least-squares and a weak learner.\n",
    "\n",
    "![Gradient tree boosting](.\\Pictures\\gradientboost.png)\n",
    "\n",
    "### Random forest vs XGBoost\n",
    "\n",
    "In theory performance should be equal\n",
    "\n",
    "- Random forest requires less tuning\n",
    "- XGBoost can be smaller\n",
    "- XGBoost is more robust to small sample sizes\n",
    "- Random forest is more efficient to train (parallel processing)\n",
    "\n",
    "## Explainability of tree-based ensembles\n",
    "\n",
    "It is difficult to interpret the models shown here:\n",
    "\n",
    "- Black box models do not show the explanation of the patterns on the outputs\n",
    "    - Neural networks, Boosting, Random forest\n",
    "- White box models do show the explanation of the patterns on the outputs\n",
    "    - Decision trees, generalized linear models\n",
    "\n",
    "Non-linear models with complex patterns will normally be black box:\n",
    "\n",
    "Some methods to make them more explainable are:\n",
    "\n",
    "- Variable importance plots\n",
    "- Shapley values\n",
    "- TreeSHAP values\n",
    "\n",
    "### Variable importance plots\n",
    "\n",
    "Shows the statistical impact of the variables in the model as measured by the Gini Index. However, they do not provide an explanation in terms of individual cases.\n",
    "\n",
    "### Shapley values\n",
    "\n",
    "A proportion between the marginal contribution of the variable to a subset of variables, divided by the number of variables in that subset, then summed so that all possible combinations of variables are considered. \n",
    "\n",
    "This is unfortunately an NP-hard calculation\n",
    "\n",
    "### TreeSHAP\n",
    "\n",
    "Using a tree-based approach, we calculate subsets of variables directly, so Shapley values can be calculated over tree cuts\n",
    "\n",
    "Maintains properties of Shapley values:\n",
    "\n",
    "- Local additivity: the shapley value of a subset of values is the sum of the values of each member of the subset\n",
    "- Consistency/monotonicity: the importance of a set of values is larger than the importance of a smaller subset of values that includes all of the original ones.\n",
    "- Missingness: if an attribute importance is zero for all subsets, its shapley value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "## Basis functions\n",
    "\n",
    "1. Can express a function as a linear combination of the basis vectors\n",
    "2. Basis vectors themselves are linearly independent\n",
    "\n",
    "### Principal component analysis\n",
    "\n",
    "This is a method to find the basis vectors that minimize the variance of the observed sample.\n",
    "\n",
    "Properties:\n",
    "\n",
    "1. Finds directions that maximize variance\n",
    "2. Directions are mutually orthogonal (linearly independent)\n",
    "3. The first component will have the highest variance\n",
    "4. The variation present in the principal components decrease as we move from the first to the last\n",
    "5. The principal components are linear combinations of the original variables/basis functions\n",
    "\n",
    "Why do we use principal component analysis/basis functions?\n",
    "\n",
    "- Data compression\n",
    "- Noise reduction\n",
    "- Feature detection for subsequent analysis\n",
    "- Visualization of multidimensional vectors\n",
    "- PCs are uncorrelated features, so correlated features can be interpreted accurate to better characterize the dataset.\n",
    "\n",
    "Formulation of PCAs:\n",
    "\n",
    "$$||x_i||^2 = ||x_i - z_iv||^2 + ||z_iv||^2$$\n",
    "\n",
    "- Length 1: $x_i - x_i^Tv$\n",
    "- Length 2: $z_i = x_i^Tv$\n",
    "- Expressed as distance of a line in space.\n",
    "- Subject to constraint that $v^Tv = 1$\n",
    "- we want to maximize variance so that it can be represented by the basis vectors\n",
    "- minimize error\n",
    "\n",
    "## Singular value decomposition\n",
    "\n",
    "![Singular value decomposition](.\\Pictures\\svd.png)\n",
    "\n",
    "- Used to isolate the forces and noise that act on a function, broke down and able to interpret each part\n",
    "- It always exists\n",
    "- U is an $n \\times k$ matrix with ortho-normal columns $UU^T = I$\n",
    "- V is an ortho-normal $k \\times k$ matrix $V^T = V^{-1}$\n",
    "- S is a $k \\times k$ diagonal matrix, with the non-negative singular values in the diagonal\n",
    "\n",
    "## Sparse PCA\n",
    "\n",
    "$$X \\simeq Z & V^T$$\n",
    "\n",
    "- Imposes a sparseness penalty $\\alpha ||V||_1$, which is equal to lasso\n",
    "- principal axes are aligned with main axes\n",
    "- latent variables are shrunken towards main axes\n",
    "\n",
    "## Latent semantic analysis (LSA)\n",
    "\n",
    "### Topic analysis\n",
    "\n",
    "Text analysis is super difficult. TF-IDF is the frequency of a word in a document, divided by frequency of documents having this term. \"the\" has high frequency, but low TF-IDF. A word like fortuitous would have a higher TF-IDF.\n",
    "\n",
    "Topic analysis is when you use a vector to analyze similarity between topics.\n",
    "\n",
    "# FINISH THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Assumes well-formed groups\n",
    "\n",
    "must be normalized\n",
    "\n",
    "## Distance-based learning\n",
    "\n",
    "- Inputs: data, closeness of instances\n",
    "- Outputs: Classifier, regressor, structure of data, results based on closeness or distance\n",
    "\n",
    "## K-NN\n",
    "\n",
    "The $k$ nearest neighbours are used as the class\n",
    "\n",
    "- Groups similar sets together\n",
    "    - Establishes prototypes, detect outliers\n",
    "    - Simplify data for further analysis\n",
    "    - Visualizes data\n",
    "\n",
    "## K-means clustering\n",
    "\n",
    "- Assumes objects are clustered in p-dimensional vectors\n",
    "- Uses distance measure between points\n",
    "- Minimizes the euclidean distance loss $\\sum_k\\sum_j ||x_j-c_i||^2$\n",
    "    - However, centroid changes for each $i$, so we need to randomly assign centroids that are fixed to assign clusters\n",
    "    - Creates **degenerate problem**: two optimal solutions to a problem, usually avoid by setting a cap\n",
    "- May not converge to an answer, will not always have the same answer\n",
    "\n",
    "Fast way to partition data into K clusters\n",
    "\n",
    "- Minimizes sum of squared euclidean distances to cluster centroids\n",
    "- Natural way to add new points to existing clusters\n",
    "- Bias towards round, equal size clusters\n",
    "\n",
    "\n",
    "### Choosing the number of clusters\n",
    "\n",
    "This is a problem that does not converge unless you have perfect parameters\n",
    "\n",
    "1. Elbow method\n",
    "    - Arbitrary selection where adding clusters does not help much\n",
    "    - Sum of variance of clusters is always smaller than variance of entire dataset\n",
    "    - Elbow exists when the curve has maximum curvature, usually the rounded number\n",
    "2. Spectral clustering\n",
    "\n",
    "## Agglomerative clustering\n",
    "\n",
    "Sequential method that does not work with large datasets\n",
    "\n",
    "- Inputs pairwise distances between a set of data objects\n",
    "- Outputs an assignment of each instance as its own cluster\n",
    "    - Find two clusters in the list that are more similar, remove both from the list, then add their union\n",
    "    - Done until the list only has one single cluster\n",
    "\n",
    "How do we decide which k is good?\n",
    "\n",
    "- starting with n steps\n",
    "- stops after n-1 steps\n",
    "\n",
    "### Cluster similarity\n",
    "\n",
    "- Single linkage\n",
    "    - prefers spatially extended, longer clusters\n",
    "- complete linkage\n",
    "    - preferes compact clusters\n",
    "- average linkage is between the two above\n",
    "\n",
    "## Spectral clustering\n",
    "\n",
    "for non-convex data\n",
    "\n",
    "- Searches clusters by pairing points\n",
    "- Clusters are well-defined, even if data is ill-shaped"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
